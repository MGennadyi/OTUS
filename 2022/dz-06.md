# Кластер Patroni on-premise.
##### 0. Установка postgresql-14.
##### 1. Установка ETCD:
```
sudo apt update && sudo apt upgrade -y
--apt purge etcd -y
--apt purge etcd-server -y
--apt purge etcd-client -y
-- apt autoremove -y
apt install etcd -y
systemctl stop etcd
rm -rf /var/lib/etcd/member/snap/*
rm -rf /var/lib/etcd/member/wal/*
ls -l /var/lib/
sudo chown -R etcd /var/lib/etcd
> /etc/default/etcd
vim /etc/default/etcd
```
###### YANDEX-ноды на внутр.статич.адресах, доменные имена полностью:
```
vim /etc/hosts
10.128.0.20 etcd1.ru-central1.internal etcd1
10.128.0.21 etcd2.ru-central1.internal etcd2
10.128.0.22 etcd3.ru-central1.internal etcd3
10.128.0.23 pg1.ru-central1.internal pg1
10.128.0.24 pg2.ru-central1.internal pg2
10.128.0.25 pg3.ru-central1.internal pg3
10.128.0.26 haproxy1.ru-central1.internal haproxy1
10.128.0.27 haproxy2.ru-central1.internal haproxy2
10.128.0.28 keepalived.ru-central1.internal keepalived
```
###### PROXMOX-ноды, статич.адреса, доменные имена полностью:
```
vim /etc/hosts
192.168.5.162 etcd1.kolomna.centr.oe	etcd1
192.168.5.163 etcd2.kolomna.centr.oe	etcd2
192.168.5.164 etcd3.kolomna.centr.oe	etcd3
192.168.5.165 pg1.kolomna.centr.oe	pg1
192.168.5.166 pg2.kolomna.centr.oe	pg2
192.168.5.167 pg3.kolomna.centr.oe	pg3
192.168.5.168 haproxy1.kolomna.centr.oe	haproxy1
192.168.5.169 haproxy2.kolomna.centr.oe	haproxy2
192.168.5.180 keepalived.kolomna.centr.oe	keepalived
```

```
# конфиг YANDEX
ETCD_NAME="etcd1.ru-central1.internal"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://10.128.0.61:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.128.0.61:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="etcd1.ru-central1.internal=http://10.128.0.61:2380,etcd2.ru-central1.internal=http://10.128.0.62:2380,etcd3.ru-central1.internal=http://10.128.0.63:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_ELECTION_TIMEOUT="5000"
ETCD_HEARTBEAT_INTERVAL="1000"
```
```
# конфиг YANDEX
ETCD_NAME="etcd2.ru-central1.internal"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://10.128.0.62:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.128.0.62:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="etcd1.ru-central1.internal=http://10.128.0.61:2380,etcd2.ru-central1.internal=http://10.128.0.62:2380,etcd3.ru-central1.internal=http://10.128.0.63:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_ELECTION_TIMEOUT="5000"
ETCD_HEARTBEAT_INTERVAL="1000"
```
```
# конфиг YANDEX
ETCD_NAME="etcd3.ru-central1.internal"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://10.128.0.63:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.128.0.63:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="etcd1.ru-central1.internal=http://10.128.0.61:2380,etcd2.ru-central1.internal=http://10.128.0.62:2380,etcd3.ru-central1.internal=http://10.128.0.63:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_ELECTION_TIMEOUT="5000"
ETCD_HEARTBEAT_INTERVAL="1000"
```
```
systemctl status etcd
systemctl start etcd
systemctl is-enabled etcd
etcdctl cluster-health
etcdctl member list
```
```
# Возможное добавление:
sudo etcdctl member add etcd4 http://192.168.5.163:2380
sudo etcdctl member add etcd5 http://192.168.5.163:2380
```
##### 2. Установка PATRONI

```
sudo passwd root
12345

sudo apt-get install gnupg1 gnupg2 -y
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
sudo wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo apt update
sudo apt-get install postgresql-14 -y
pg_isready
systemctl status postgresql
-- systemctl start postgresql
-- systemctl stop postgresql
sudo apt-get install -y python3 python3-pip git
sudo pip3 install psycopg2-binary
systemctl stop postgresql
sudo -u postgres pg_dropcluster 14 main
sudo systemctl daemon-reload
pg_dropcluster 14 main
# Ответ:
Error: specified cluster does not exist
pg_isready
# Ответ:
/var/run/postgresql:5432 - no response
pg_lsclusters
sudo pip3 install patroni[etcd]
sudo ln -s /usr/local/bin/patroni /bin/patroni
```
```
sudo vim /etc/systemd/system/patroni.service
```
```
[Unit]
Description=High availability PostgreSQL Cluster
After=syslog.target network.target

[Service]
Type=simple
User=postgres
Group=postgres
ExecStart=/usr/local/bin/patroni /etc/patroni.yml
KillMode=process
TimeoutSec=30
Restart=always

[Install]
WantedBy=multi-user.target
```
###### Вставляем в patroni.yml свой вариант конфига для каждой ноды.
```
sudo vim /etc/patroni.yml
```
###### Бутстрапим:
```
sudo -u postgres patroni /etc/patroni.yml
```
```
# Ответ:
ERROR: Failed to get list of machines from http://etcd1:2379/v2: 
2022-08-17 10:45:56,712 ERROR: Failed to get list of machines from http://etcd2:2379/v2: 
2022-08-17 10:45:56,714 ERROR: Failed to get list of machines from http://etcd3:2379/v2: 
```
###### Исправляем ошибку. В секции postgresql указываем путь к бинарникам: bin_dir: /usr/lib/postgresql/14/bin
###### Ответ: INFO: no action. I am (pg3), a secondary, and following a leader (pg1)
###### Проверим, изменим состояние запуска patroni и стартуем его как сервис:
```
sudo systemctl is-enabled patroni
sudo systemctl enable patroni
# Ответ: Created symlink /etc/systemd/system/multi-user.target.wants/patroni.service → /etc/systemd/system/patroni.service.
sudo systemctl restart patroni
sudo systemctl status patroni
# ВАЖНО:  не удалось начать трансляцию WAL: ОШИБКА:  слот репликации "pg3" не существует
sudo patronictl -c /etc/patroni.yml list
```
Member|Host|Role|State|Tl|Lag in MB|
:----|:--------:|-----:|-----:|-----:|-----:
pg1|192.168.5.165|Leader  |Running|1| |
pg2|192.168.5.166|Replica |Running|1|0|
pg3|192.168.5.167|Replica |Running|1|0|
```
sudo patronictl -c /etc/patroni.yml restart postgres
Error: postgres cluster doesn't have any members
```
###### Если поронять: на мастере pg1:
```
systemctl stop patroni
```
###### На pg2:
```
root@pg2:/home/mgb# sudo patronictl -c /etc/patroni.yml list
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Replica | stopped |    |   unknown |
| pg2    | 192.168.5.166 | Leader  | running |  3 |           |
| pg3    | 192.168.5.167 | Replica | running |  3 |         0 |
+--------+---------------+---------+---------+----+-----------+
root@pg2:/home/mgb# sudo patronictl -c /etc/patroni.yml list
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg2    | 192.168.5.166 | Leader  | running |  3 |           |
| pg3    | 192.168.5.167 | Replica | running |  3 |         0 |
+--------+---------------+---------+---------+----+-----------+
```
###### На выбывшем мастере pg1:
```
systemctl start patroni
sudo patronictl -c /etc/patroni.yml list
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Replica | running | 10 |         0 |
| pg2    | 192.168.5.166 | Leader  | running | 10 |           |
| pg3    | 192.168.5.167 | Replica | running | 10 |         0 |
+--------+---------------+---------+---------+----+-----------+
```
###### Patroni отработал потерю и возвращение одной ноды. Далее:

### 3. Pg_Bouncer. 
```
# Создается на всех 3-х нодах:
sudo apt install -y pgbouncer
sudo systemctl status pgbouncer
sudo systemctl stop pgbouncer
# логи пишутся в другое место, 2 позиции пропускаем:
-- sudo mkdir /var/log/pgbouncer
-- sudo chown -R postgres /var/log/pgbouncer
vim /etc/pgbouncer/pgbouncer.ini
```
```
[databases]
otus = host=127.0.0.1 port=5432 dbname=otus 
[pgbouncer]
logfile = /var/log/postgresql/pgbouncer.log
pidfile = /var/run/postgresql/pgbouncer.pid
listen_addr = *
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
admin_users = admindb
# В дальнейшем Аристов заменил на 
admin_users = postgres
```
###### На pg3
```
sudo -u postgres psql -h localhost
# Пробуем создать базу
create database otus;
# ОШИБКА: режим "только чтение" - это же реплика!
```
###### На master pg2:
```
sudo -u postgres psql -h localhost
create database otus;
```
###### Проверка создания БД OTUS на реплике на pg3. 
```
\l
    Имя    | Владелец | Кодировка | LC_COLLATE  |  LC_CTYPE   |     Права доступа
-----------+----------+-----------+-------------+-------------+-----------------------
 otus      | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 |
 postgres  | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 |
 template0 | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 | =c/postgres          +
           |          |           |             |             | postgres=CTc/postgres
 template1 | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 | =c/postgres          +
           |          |           |             |             | postgres=CTc/postgres
```
###### На всех нодах добавляем пароль admindb в формате md5 в userlist.txt
```
sudo -u postgres psql -h localhost
create user admindb with password '12345';
\du
select * from pg_user;
select usename, passwd from pg_shadow;
# Копируем хеш пароля
vim /etc/pgbouncer/userlist.txt
"postgres"  "SCRAM-SHA-256$4096:5QzE5QKo/Y0sUSKxIIhmBA==$mViw6DFyxLkcRRDyft3UOoJ/JGw5zRyvpOJnkt+EtmI=:8gi9vxsVajxpmcVZb3gqjnecPeXoel/eBqk3pEbbX7k="
```
###### Как запущен pgbouncer? Как демон или служба?
```
su postgres
ps -xf
3286 ?        Ssl    0:02 /usr/sbin/pgbouncer /etc/pgbouncer/pgbouncer.ini
kill 3286
sudo systemctl start pgbouncer
su postgres
ps -xf
3996 ?        Ssl    0:00 /usr/sbin/pgbouncer /etc/pgbouncer/pgbouncer.ini
sudo -u postgres -p 6432 -h 127.0.0.1 otus
sudo -u postgres psql -p 6432 -h 127.0.0.1 otus
# ввести passwd= zalando_321
```
###### Нагрузим с pg1 на pg2  192.168.5.166:
```
sudo -u postgres pgbench -p 6432 -c 20 -C -T 60 -P 1 -d otus -h 192.168.5.166

# Аристов: если указать не сущ.бд, то pgbouncer умрет. Небходимо включить рестарт сервиса при падении Restart=always
```
###### Админка pgbouncera
```
sudo -u postgres psql -p 6432 pgbouncer -h localhost
show clients;
 type |   user   | database  | state  | addr | port  | local_addr | local_port |      connect_time       |      request_time       | wait | wait_us | close_needed |      ptr       | link | remote_pid | tls
------+----------+-----------+--------+------+-------+------------+------------+-------------------------+-------------------------+------+---------+--------------+----------------+------+------------+-----
 C    | postgres | pgbouncer | active | ::1  | 51976 | ::1        |       6432 | 2022-07-01 13:35:24 MSK | 2022-07-01 13:35:36 MSK |    0 |       0 |            0 | 0x564fa861a780 |      |          0 |

```

###### Тестим на switchover:
```
sudo patronictl -c /etc/patroni.yml switchover
Master [pg2]:
Candidate ['pg1', 'pg3'] []: pg1
When should the switchover take place (e.g. 2022-07-01T14:38 )  [now]:
Current cluster topology
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Replica | running | 10 |         0 |
| pg2    | 192.168.5.166 | Leader  | running | 10 |           |
| pg3    | 192.168.5.167 | Replica | running | 10 |         0 |
+--------+---------------+---------+---------+----+-----------+
Are you sure you want to switchover cluster patroni, demoting current master pg2? [y/N]: y
2022-07-01 13:39:16.67253 Successfully switched over to "pg1"
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Leader  | running | 10 |           |
| pg2    | 192.168.5.166 | Replica | stopped |    |   unknown |
| pg3    | 192.168.5.167 | Replica | running | 10 |         0 |
+--------+---------------+---------+---------+----+-----------+
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Leader  | running | 11 |           |
| pg2    | 192.168.5.166 | Replica | running | 11 |         0 |
| pg3    | 192.168.5.167 | Replica | running | 11 |         0 |
+--------+---------------+---------+---------+----+-----------+
```
###### Переключение master с pg2 на pg1 произведено успешно.

### 4. HAPROXY
###### С новых 2-х нод тестим ответ :
```
curl -v 192.168.5.165:8008/replica
# HTTP/1.0 200 OK
curl -v 192.168.5.166:8008/master
# HTTP/1.0 200 OK
curl -v 192.168.5.167:8008/replica
# HTTP/1.0 200 OK
```
```
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
sudo wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo apt update && sudo apt upgrade -y
sudo apt install postgresql-client-common postgresql-client -y
sudo apt install -y haproxy
sudo vim /etc/haproxy/haproxy.cfg
```
```
listen postgres_write
    bind *:5432
    mode            tcp
    option httpchk
    http-check connect
    http-check send meth GET uri /master
    http-check expect status 200
    default-server inter 10s fall 3 rise 3 on-marked-down shutdown-sessions
    server pg1 192.168.5.165:6432 check port 8008
    server pg2 192.168.5.166:6432 check port 8008
    server pg3 192.168.5.167:6432 check port 8008

listen postgres_read
    bind *:5433
    mode            tcp
    http-check connect
    http-check send meth GET uri /replica
    http-check expect status 200
    default-server inter 10s fall 3 rise 3 on-marked-down shutdown-sessions
    server pg1 192.168.5.165:6432 check port 8008
    server pg2 192.168.5.166:6432 check port 8008
    server pg3 192.168.5.167:6432 check port 8008
```
```
# Проверки:
haproxy -f /etc/haproxy/haproxy.cfg -c
sudo systemctl restart haproxy.service
sudo systemctl status haproxy.service
# Ответ:
июл 05 10:31:44 haproxy1 haproxy[3466]: Proxy postgres_write started.
июл 05 10:31:44 haproxy1 haproxy[3466]: Proxy postgres_read started.
июл 05 10:31:44 haproxy1 systemd[1]: Started HAProxy Load Balancer.
```
```
# Проверим, что все рабочее: c haproxy1 или haproxy2 :
psql -p 6432 -d otus -h 192.168.5.166 -U postgres
create database haproxy;
\q
# Подключение без указания master/replica:
psql -h localhost -d otus -U postgres -p 5432
create database haproxy1;
```
### 5.  KEEPALIVED
###### 1. На haproxy1, haproxy2 установим и вкл. вирт.сетевой адаптер, по умолч. откл:
```
sudo apt install -y keepalived
sudo vim /etc/sysctl.conf
net.ipv4.ip_nonlocal_bind=1
# устраненение ошибки в статусе:
groupadd -r keepalived_script
useradd -r -s /sbin/nologin -g keepalived_script -M keepalived_script
# применим изменения
sudo sysctl -p
ip a
# адрес сетевого интерфейса: ens18
sudo vim /etc/keepalived/keepalived.conf
# Вставляем свой keepalived.conf и keepalived2.conf и указываем virtual_ipadress: 192.168.5.180 с равными вирт.роутер id
# Секция сбора статистики:
frontend stats
    bind *:9000
    stats enable
    stats uri /stats
    stats refresh 10s
    stats auth haproxy:haproxy
```    
```
sudo service keepalived start
sudo service keepalived status
```
```
# Ответ haproxy1
июл 05 12:28:58 haproxy1 Keepalived_vrrp[3929]: (VI_01) ip address associated with VRID 51 not present in MASTER advert : 192.168.5.180
июл 05 12:28:59 haproxy1 Keepalived_vrrp[3929]: (VI_01) Received advert from 192.168.5.169 with lower priority 100, ours 111, forcing new election
```
```
# Ответ haproxy2
июл 05 12:28:59 haproxy2 Keepalived_vrrp[1477]: (VI_01) Entering MASTER STATE
июл 05 12:28:59 haproxy2 Keepalived_vrrp[1477]: (VI_01) Master received advert from 192.168.5.168 with higher priority 111, ours 100
июл 05 12:28:59 haproxy2 Keepalived_vrrp[1477]: (VI_01) Entering BACKUP STATE
```
```
apt install nmap
 nmap -v -Pn 192.168.5.180 -p 9000
# Ответ:
PORT     STATE SERVICE
9000/tcp open  cslistener
MAC Address: EA:39:5F:71:63:86 (Unknown)
```
```
psql -p 5432 -d otus -h 192.168.5.180 -U postgres
# Ответ:
Пароль пользователя postgres:
psql (14.4 (Debian 14.4-1.pgdg110+1))
Введите "help", чтобы получить справку.
otus=#
```
```
# Просмотр статистики haproxy:
http://192.168.5.180:9000/stats
```




















###### !!! Если поломался старый кластер
```
patronictl -c /etc/patroni.yml remove 7088634863084761990
```
