# Кластер Patroni on-premise.
```
# настройки ОС:
sudo passwd root
12345
apt install mc vim network-manager -y
nmtui
sudo apt update && sudo apt upgrade -y
# Уст. доп.модули
# sudo apt-get install -y gnupg1
# sudo apt-get install -y gnupg2
```

### 0. Удаление ETCD, если надо:
```
apt purge etcd -y
apt purge etcd-server -y
apt purge etcd-client -y
apt autoremove -y
# Удаление записей о PATRONI в ETCD, где /patroni - это имя scope: patroni:
systemctl stop etcd
systemctl status etcd
etcdctl rm /service/patroni --recusive
```
### 1. Установка ETCD:
```
apt install etcd -y
systemctl status etcd
● etcd.service - etcd - highly-available key value store
     Loaded: loaded (/lib/systemd/system/etcd.service; enabled; vendor preset: enabled)
     Active: active (running) since Thu 2024-04-18 13:58:58 MSK; 1min 3s ago
etcd -version
etcd Version: 3.3.25
```
###
```
# Совет от big, кластер не соберется, если не удалить:
systemctl stop etcd
rm -rf /var/lib/etcd/member/snap/*
rm -rf /var/lib/etcd/member/wal/*
ls -l /var/lib/
# ETCD при установке пропишет нового пользователя etcd, поэтому дадим ему права на свой каталог:
sudo chown -R etcd /var/lib/etcd
> /etc/default/etcd
vim /etc/default/etcd
# Вставляем свой вариант конфига и стартуем
systemctl start etcd
# Проверка состояния:
systemctl status etcd
etcdctl cluster-health
# Ответ на ETCD1:
member 8a1503215af0004 is healthy: got healthy result from http://192.168.5.163:2379
member 2ea762322acf6202 is healthy: got healthy result from http://192.168.5.162:2379
member 657a08b74f094256 is healthy: got healthy result from http://192.168.5.164:2379
etcdctl member list
# Ответ ETCD1:
8a1503215af0004: name=etcd2 peerURLs=http://192.168.5.163:2380 clientURLs=http://192.168.5.163:2379 isLeader=false
2ea762322acf6202: name=etcd1 peerURLs=http://192.168.5.162:2380 clientURLs=http://192.168.5.162:2379 isLeader=true
657a08b74f094256: name=etcd3 peerURLs=http://192.168.5.164:2380 clientURLs=http://192.168.5.164:2379 isLeader=false
```
#### YANDEX-ноды на внутр.статич.адресах, доменные имена полностью:
```
vim /etc/hosts
#
10.128.0.61 etcd1.ru-central1.internal etcd1
10.128.0.62 etcd2.ru-central1.internal etcd2
10.128.0.63 etcd2.ru-central1.internal etcd3
10.128.0.64 patroni1.ru-central1.internal patroni1
10.128.0.65 patroni2.ru-central1.internal patroni2
10.128.0.66 patroni3.ru-central1.internal patroni3
10.128.0.67 haproxy1.ru-central1.internal	haproxy1
10.128.0.68 haproxy2.ru-central1.internal	haproxy2
10.128.0.69 keepalived.ru-central1.internal	keepalived
```
#### PROXMOX-ноды, статич.адреса, доменные имена полностью:
```
vim /etc/hosts
192.168.5.162 etcd1.kolomna.centr.oe	etcd1
192.168.5.163 etcd2.kolomna.centr.oe	etcd2
192.168.5.164 etcd3.kolomna.centr.oe	etcd3
192.168.5.165 pg1.kolomna.centr.oe	pg1
192.168.5.166 pg2.kolomna.centr.oe	pg2
192.168.5.167 pg3.kolomna.centr.oe	pg3
192.168.5.168 haproxy1.kolomna.centr.oe	haproxy1
192.168.5.169 haproxy2.kolomna.centr.oe	haproxy2
192.168.5.180 keepalived.kolomna.centr.oe	keepalived
```
#### Virtual-box-ноды :
```
vim /etc/hosts
192.168.0.141 etcd1
192.168.0.142 etcd2
192.168.0.143 etcd3
192.168.0.144 patroni1
192.168.0.145 patroni2
192.168.0.146 patrony3
192.168.0.147 haproxy1
192.168.0.148 haproxy2
192.168.0.149 keepalived
```
#### конфиг YANDEX
```
systemctl stop etcd
systemctl status etcd
rm -rf /var/lib/etcd/member/snap/*
rm -rf /var/lib/etcd/member/wal/*
> /etc/default/etcd
vim /etc/default/etcd
# ETCD при установке пропишет нового пользователя etcd, поэтому дадим ему права на свой каталог:
sudo chown -R etcd /var/lib/etcd
systemctl start etcd
systemctl status etcd
etcdctl cluster-health
etcdctl member list
```
#### YANDEX обращение к etcd1,2,3 через ip-адреса:
```
ETCD_NAME="etcd1"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://10.128.0.61:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.128.0.61:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="etcd1=http://10.128.0.61:2380,etcd2=http://10.128.0.62:2380,etcd3=http://10.128.0.63:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_ELECTION_TIMEOUT="5000"
ETCD_HEARTBEAT_INTERVAL="1000"
```
```
# ETCD2 конфиг YANDEX
ETCD_NAME="etcd2"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://10.128.0.62:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.128.0.62:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="etcd1=http://10.128.0.61:2380,etcd2=http://10.128.0.62:2380,etcd3=http://10.128.0.63:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_ELECTION_TIMEOUT="5000"
ETCD_HEARTBEAT_INTERVAL="1000"
```
```
# ETCD3 конфиг YANDEX
ETCD_NAME="etcd3"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://10.128.0.63:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.128.0.63:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="etcd1=http://10.128.0.61:2380,etcd2=http://10.128.0.62:2380,etcd3=http://10.128.0.63:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_ELECTION_TIMEOUT="5000"
ETCD_HEARTBEAT_INTERVAL="1000"
```
#### Проверка состояния ETCD:
```
systemctl start etcd
systemctl status etcd
systemctl start etcd
systemctl is-enabled etcd
# Ответ: enabled
etcdctl cluster-health
# Ответ: 
member 8a1503215af0004 is healthy: got healthy result from http://192.168.5.163:2379
member 2ea762322acf6202 is healthy: got healthy result from http://192.168.5.162:2379
member 657a08b74f094256 is healthy: got healthy result from http://192.168.5.164:2379
etcdctl member list
# Ответ:
8a1503215af0004: name=etcd2 peerURLs=http://192.168.5.163:2380 clientURLs=http://192.168.5.163:2379 isLeader=true
2ea762322acf6202: name=etcd1 peerURLs=http://192.168.5.162:2380 clientURLs=http://192.168.5.162:2379 isLeader=false
657a08b74f094256: name=etcd3 peerURLs=http://192.168.5.164:2380 clientURLs=http://192.168.5.164:2379 isLeader=false
```
###### Проверка доступности нод по имени:
```
ping etcd1.ru-central1.internal
ping etcd2.ru-central1.internal
ping etcd3.ru-central1.internal
ping patroni1.ru-central1.internal
ping patroni2.ru-central1.internal
ping patroni3.ru-central1.internal
```
```
# Возможное добавление:
sudo etcdctl member add etcd4 http://192.168.5.163:2380
sudo etcdctl member add etcd5 http://192.168.5.163:2380
```
### 2.1 Установка postgresql-14.
```
# Установим 14-ю версию postgresql:
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
# В pgdg.list появится запись: deb http://apt.postgresql.org/pub/repos/apt/ bullseye-pgdg main
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
# Ответ:
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
OK
sudo apt update
apt install postgresql-14 -y
# Проверка установку:
systemctl status postgresql
pg_lsclusters
pg_isready
# Ответ:
/var/run/postgresql:5432 - accepting connections
```
#### 2.2 Установка postgresql-15.
```
# Установим последнюю 15-ю версию postgresql:
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main 15" > /etc/apt/sources.list.d/pgdg.list'
vim /etc/apt/sources.list.d/pgdg.list
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo apt update
sudo apt-get -y install postgresql-15
# Ответ:
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
# Решение: https://zalinux.ru/?p=5066

# Проверка установку:
systemctl status postgresql
pg_lsclusters
pg_isready
# Ответ:
/var/run/postgresql:5432 - accepting connections
```

#### 2. Установка PATRONI - заставка
#### Перед установкой патрони необходимо выполнить предварительные действия:
```
# 1. Установить ПО питон :
sudo apt-get install -y python3 python3-pip git
# 2. Уст. модули для pyton:
sudo pip3 install psycopg2-binary
# 3 Останавить 14 postgresql:
systemctl stop postgresql@14-main
# Postgres, как самостоятельный сервис больше не должен запускаться # Отключаем - совет от big::
systemctl disable postgresql
# 4. Проверим остановленный кластер:
pg_lsclusters
# Ответ: 
Ver Cluster Port Status Owner    Data directory              Log file
14  main    5432 down   postgres /var/lib/postgresql/14/main /var/log/postgresql/postgresql-14-main.log
```
#### 3. Удалить кластер 14 main:
```
sudo -u postgres pg_dropcluster 14 main
# pg_dropcluster 14 main
# Убеждаемся, что все удалилось:
pg_lsclusters
# Ответ: 
Ver Cluster Port Status Owner    Data directory              Log file
Еще одна проверка:
pg_isready
# Ответ: /var/run/postgresql:5432 - no response
```
##### Теперь можно устанавливать Patroni с модулем etcd :
```
sudo pip3 install patroni[etcd]
# Делаем сим-линк:
sudo ln -s /usr/local/bin/patroni /bin/patroni
```
###### Включаем старт сервиса:
```
sudo vim /etc/systemd/system/patroni.service
```
```
[Unit]
Description=High availability PostgreSQL Cluster
After=syslog.target network.target

[Service]
Type=simple
User=postgres
Group=postgres
ExecStart=/usr/local/bin/patroni /etc/patroni.yml
KillMode=process
TimeoutSec=30
Restart=always

[Install]
WantedBy=multi-user.target
```
#### Вставляем в patroni.yml свой вариант конфига для каждой ноды.
```
sudo vim /etc/patroni.yml
```
#### Запуск с ошибкой: Бутстрапим PATRONI. Сначала старт сервис!!! , потом бутстрап :
```
# Возможно придется изменить на полные имена:
hosts: etcd1.ru-central1.internal:2379,etcd2.ru-central1.internal:2379,etcd3.ru-central1.internal:2379
```

#### 1. Запуск PATRONI как сервис. При старте сам PATRONI создаст кластер POSTGRES:
```
# Однако patroni по умолчанию имеет статус disabled, проверим это:
sudo systemctl is-enabled patroni
# Ответ: disabled. Поэтому делаем его enabled:
sudo systemctl enable patroni
# Ответ: Created symlink /etc/systemd/system/multi-user.target.wants/patroni.service → /etc/systemd/system/patroni.service.
sudo systemctl start patroni
sudo systemctl status patroni
# Ответ:
авг 19 11:47:32 pg1 systemd[1]: Started High availability PostgreSQL Cluster.
авг 19 11:47:32 pg1 patroni[839]: 2022-08-19 11:47:32,302 INFO: Selected new etcd server http://192.168.5.163:2379
авг 19 11:47:32 pg1 patroni[839]: 2022-08-19 11:47:32,308 INFO: No PostgreSQL configuration items changed, nothing to reload.
авг 19 11:47:32 pg1 patroni[839]: 2022-08-19 11:47:32,312 INFO: Lock owner: None; I am pg1
авг 19 11:47:32 pg1 patroni[839]: 2022-08-19 11:47:32,316 INFO: waiting for leader to bootstrap
● patroni.service - High availability PostgreSQL Cluster
     Loaded: loaded (/etc/systemd/system/patroni.service; enabled; vendor preset: enabled)
     Active: active (running) since Wed 2022-08-24 13:55:09 MSK; 10s ago
   Main PID: 738 (patroni)
      Tasks: 9 (limit: 2324)
     Memory: 86.4M
        CPU: 1.406s
     CGroup: /system.slice/patroni.service
             ├─738 /usr/bin/python3 /usr/local/bin/patroni /etc/patroni.yml
             ├─744 /usr/lib/postgresql/14/bin/pg_ctl initdb -D /var/lib/postgresql/14/main -o --encoding=UTF8 --data-checksums --username=postgres --pwfile=/tmp/tmpon84tu2i
             ├─747 sh -c "/usr/lib/postgresql/14/bin/initdb" -D "/var/lib/postgresql/14/main" --encoding=UTF8 --data-checksums --username=postgres --pwfile=/tmp/tmpon84tu2i
             └─748 /usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --encoding=UTF8 --data-checksums --username=postgres --pwfile=/tmp/tmpon84tu2i

авг 24 13:55:10 pg1 patroni[748]: Контроль целостности страниц данных включён.
авг 24 13:55:10 pg1 patroni[748]: создание каталога /var/lib/postgresql/14/main... ок
авг 24 13:55:10 pg1 patroni[748]: создание подкаталогов... ок
авг 24 13:55:10 pg1 patroni[748]: выбирается реализация динамической разделяемой памяти... posix
авг 24 13:55:10 pg1 patroni[748]: выбирается значение max_connections по умолчанию... 100
авг 24 13:55:10 pg1 patroni[748]: выбирается значение shared_buffers по умолчанию... 128MB
авг 24 13:55:10 pg1 patroni[748]: выбирается часовой пояс по умолчанию... Europe/Moscow
авг 24 13:55:10 pg1 patroni[748]: создание конфигурационных файлов... ок
авг 24 13:55:11 pg1 patroni[748]: выполняется подготовительный скрипт... ок
авг 24 13:55:12 pg1 patroni[748]: выполняется заключительная инициализация... ок
# Сделал скрин!
# Проверм состояние:
sudo patronictl -c /etc/patroni.yml list
root@pg1:/home/mgb# sudo patronictl -c /etc/patroni.yml list
+--------+---------------+--------+---------+----+-----------+
| Member | Host          | Role   | State   | TL | Lag in MB |
+ Cluster: patroni (7135394571389649652) ---+----+-----------+
| pg1    | 192.168.5.165 | Leader | running |  1 |           |
+--------+---------------+--------+---------+----+-----------+
# Проверка уст.версии Patroni:
patronictl version
```
#### Старт сервиса на pg3
```
root@pg3:/home/mgb# sudo systemctl start patroni
root@pg3:/home/mgb# sudo systemctl status patroni
● patroni.service - High availability PostgreSQL Cluster
     Loaded: loaded (/etc/systemd/system/patroni.service; enabled; vendor preset: enabled)
     Active: active (running) since Wed 2022-08-24 15:36:42 MSK; 13s ago
   Main PID: 10580 (patroni)
      Tasks: 7 (limit: 2324)
     Memory: 69.6M
        CPU: 477ms
     CGroup: /system.slice/patroni.service
             ├─10580 /usr/bin/python3 /usr/local/bin/patroni /etc/patroni.yml
             └─10586 /usr/lib/postgresql/14/bin/pg_basebackup --pgdata=/var/lib/postgresql/14/main -X stream --dbname=dbname=postgres user=replicator host=192.168.5.165 port=5432
авг 24 15:36:42 pg3 systemd[1]: Started High availability PostgreSQL Cluster.
авг 24 15:36:43 pg3 patroni[10580]: 2022-08-24 15:36:43,325 INFO: Selected new etcd server http://192.168.5.163:2379
авг 24 15:36:43 pg3 patroni[10580]: 2022-08-24 15:36:43,332 INFO: No PostgreSQL configuration items changed, nothing to reload.
авг 24 15:36:43 pg3 patroni[10580]: 2022-08-24 15:36:43,366 INFO: Lock owner: pg1; I am pg3
авг 24 15:36:43 pg3 patroni[10580]: 2022-08-24 15:36:43,450 INFO: trying to bootstrap from leader 'pg1'
авг 24 15:36:44 pg3 patroni[10586]: ПРЕДУПРЕЖДЕНИЕ:  специальный файл "./.s.PGSQL.5432" пропускается
авг 24 15:36:44 pg3 patroni[10586]: ПРЕДУПРЕЖДЕНИЕ:  специальный файл "./.s.PGSQL.5432" пропускается
авг 24 15:36:53 pg3 patroni[10580]: 2022-08-24 15:36:53,337 INFO: Lock owner: pg1; I am pg3
авг 24 15:36:53 pg3 patroni[10580]: 2022-08-24 15:36:53,501 INFO: bootstrap from leader 'pg1' in progress
# Сделал скрин
```
#### 2. Запуск Бутстрап PATRONI:
```
sudo -u postgres patroni /etc/patroni.yml
# Пошло создавание кластера
# ВАЖНО:  не удалось начать трансляцию WAL: ОШИБКА:  слот репликации "pg3" не существует
###### Бутстрапим PATRONI еще раз:
```
Member|Host|Role|State|Tl|Lag in MB|
:----|:--------:|-----:|-----:|-----:|-----:
pg1|192.168.5.165|Leader  |Running|1| |
pg2|192.168.5.166|Replica |Running|1|0|
pg3|192.168.5.167|Replica |Running|1|0|
```
sudo patronictl -c /etc/patroni.yml restart postgres
Error: postgres cluster doesn't have any members
# После вннесения измен: Перегрузка всего кластера: Где: CLUSTER_NAME=patroni
 sudo patronictl -c /etc/patroni.yml reload patroni
 sudo patronictl -c /etc/patroni.yml restart patroni
 
```
#### А если поронять: к примеру остановить лидер pg1:
```
systemctl stop patroni
```
###### На pg2:
```
root@pg2:/home/mgb# sudo patronictl -c /etc/patroni.yml list
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Replica | stopped |    |   unknown |
| pg2    | 192.168.5.166 | Leader  | running |  3 |           |
| pg3    | 192.168.5.167 | Replica | running |  3 |         0 |
+--------+---------------+---------+---------+----+-----------+
root@pg2:/home/mgb# sudo patronictl -c /etc/patroni.yml list
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg2    | 192.168.5.166 | Leader  | running |  3 |           |
| pg3    | 192.168.5.167 | Replica | running |  3 |         0 |
+--------+---------------+---------+---------+----+-----------+
```
###### На выбывшем мастере pg1:
```
systemctl start patroni
sudo patronictl -c /etc/patroni.yml list
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Replica | running | 10 |         0 |
| pg2    | 192.168.5.166 | Leader  | running | 10 |           |
| pg3    | 192.168.5.167 | Replica | running | 10 |         0 |
+--------+---------------+---------+---------+----+-----------+
```
###### Patroni отработал потерю и возвращение одной ноды: failover. Далее: ручной switchover:
```
sudo patronictl -c /etc/patroni.yml switchover
# Ответ: Master [pg1]: pg1
```

### 3. Pg_Bouncer - легковесный пул соединений. 2кб на соединение
```
# Создается на всех 3-х нодах:
sudo apt install -y pgbouncer
sudo systemctl status pgbouncer
# Как и с ETCD правим конфиг на выключенном сервисе:
sudo systemctl stop pgbouncer
vim /etc/pgbouncer/pgbouncer.ini
```
```
[databases]
otus = host=127.0.0.1 port=5432 dbname=otus 
* = host=127.0.0.1 port=5432 
[pgbouncer]
logfile = /var/log/postgresql/pgbouncer.log
pidfile = /var/run/postgresql/pgbouncer.pid
listen_addr = *
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
# admin_users = admindb - для доступа к админке
# В дальнейшем Аристов заменил на единого пользователя 
admin_users = postgres
```
##### Проверить команду:
```
show pools;
show pools
```
### Pgbouncer - рестарт при падении:
```
sudo vim /lib/systemd/system/pgbouncer.service
# В секции service:
Restart=always
```
###### На всех нодах добавляем пароль admindb, postgres в формате md5 в userlist.txt
###### Документация: пароли в userlist.txt хранятся в md5, однако вставляем из pg_shadow SCRAM-SHA-256 и проходит.
```
# из-под postgres:
sudo -u postgres psql -h localhost
CREATE USER admindb with password '12345';
ALTER USER postgres WITH PASSWORD '12345';
# DROP USER admindb;
\du
postgres=# select * from pg_user;
 usename  | usesysid | usecreatedb | usesuper | userepl | usebypassrls |  passwd  | valuntil | useconfig
----------+----------+-------------+----------+---------+--------------+----------+----------+-----------
 admindb  |    16411 | f           | f        | f       | f            | ******** |          |
 postgres |       10 | t           | t        | t       | t            | ******** |          |
# Добраться до Хешь пароля только так:
postgres=# select usename, passwd from pg_shadow;
 usename  |                                                                passwd
----------+---------------------------------------------------------------------------------------------------------------------------------------
 admindb  | SCRAM-SHA-256$4096:3oC2ZRzS/2Q8pBwrmCToNw==$HYTbhLdPZhMxA9y18vKcUlE+1X7b1ZYmJsmnlgpVD58=:427lPsl99p+jttxRoX/C5peCepRcqpijyNuWN387D2c=
 postgres | SCRAM-SHA-256$4096:USpSx+bi66bSqinvndEHuw==$jjPvuACPkFHjDqD6b2hyvcP9CgfXyIP3h6p45dUh23o=:M2ZuVRCcRdfBxhiDjuVJRiWbh9NBAqZ+nAtKssNg3yU=
# Прописываем полченный хеш пароля в userlist.txt: нюанс: pass=12345:
vim /etc/pgbouncer/userlist.txt
# хэш Аристова - подходит:
"postgres" "SCRAM-SHA-256$4096:5QzE5QKo/Y0sUSKxIIhmBA==$mViw6DFyxLkcRRDyft3UOoJ/JGw5zRyvpOJnkt+EtmI=:8gi9vxsVajxpmcVZb3gqjnecPeXoel/eBqk3pEbbX7k="
# хэш мой из pg_shadow - подходит passwd= zalando_321:
"postgres" "SCRAM-SHA-256$4096:2NrutDh12wiaJfL+AehCIg==$xjeJzonnSFfA5X5OCYvwwv5g/CC1kNuxpev5oJcfxrw=:khSkYStWN0Pnq4Qcu3ZhUnPfDs7vkrbrxfjQNgRtHJA="
```
```
sudo systemctl daemon-reload
sudo systemctl start pgbouncer
sudo systemctl status pgbouncer
```
```
sudo -u postgres psql -p 6432 -h 127.0.0.1 otus
# вводим pass
otus=#
```

###### На pg1 в role=Leader:
```
sudo -u postgres psql -h localhost
create database otus;
# Ответ: CREATE DATABASE
```
###### На pg3 в role=Replica:
```
sudo -u postgres psql -h localhost
# Пробуем создать базу
create database otus;
# Ответ: ОШИБКА: режим "только чтение"
```
###### На master pg2:
```
sudo -u postgres psql -h localhost
create database otus;
```
###### Проверка создания БД OTUS на реплике на pg3. 
```
\l
    Имя    | Владелец | Кодировка | LC_COLLATE  |  LC_CTYPE   |     Права доступа
-----------+----------+-----------+-------------+-------------+-----------------------
 otus      | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 |
 postgres  | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 |
 template0 | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 | =c/postgres          +
           |          |           |             |             | postgres=CTc/postgres
 template1 | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 | =c/postgres          +
           |          |           |             |             | postgres=CTc/postgres
```

###### Как запущен pgbouncer? Как демон или служба?
```
su postgres
ps -xf
3286 ?        Ssl    0:02 /usr/sbin/pgbouncer /etc/pgbouncer/pgbouncer.ini
kill 3286
sudo systemctl start pgbouncer
su postgres
ps -xf
3996 ?        Ssl    0:00 /usr/sbin/pgbouncer /etc/pgbouncer/pgbouncer.ini
sudo -u postgres -p 6432 -h 127.0.0.1 otus
sudo -u postgres psql -p 6432 -h 127.0.0.1 otus
# ввести passwd= zalando_321
```
##### Рестарт pgbouncer при падении
```
# Добавить в секции "service":
sudo vim /lib/systemd/system/pgbouncer.service
Restart=always
# рестарт 
systemctl daemon-reload
sudo systemctl restart pgbouncer

```
##### 4. PGBENCH с pg1 на pg2  192.168.5.166:
```
# 1. Создадим тестовую базу на pg1:
sudo -u postgres psql -h localhost
create database testpgbench;
\l
# 2. Подготовим тестовую  базу testpgbench для тестов: 
sudo -u postgres pgbench -i -s 10 -F 80 -U postgres testpgbench
# sudo -u postgres pgbench -p 6432 -c 20 -C -T 60 -P 1 -d testpgbench -h 192.168.5.166
sudo -u postgres pgbench -h localhost -p 5432 -d testpgbench -c 20 -C -T 60 -P 1
# Ответ:
latency average = 536.734 ms
latency stddev = 533.979 ms
average connection time = 3.890 ms
tps = 36.399547 (including reconnection times)
# -P 60 (отображение прогресса каждые 60 сек)
sudo -u postgres pgbench -h localhost -p 5432 -d testpgbench -c 200 -C -T 60 -P 60
# Ответ:
average connection time = 4.254 ms
tps = 52.098236
sudo -u postgres pgbench -h 192.168.5.180 -p 5432 -d testpgbench -U postgres -c 200 -C -T 60 -P 60
# Использование сценария скрипт на чтение:
vim readonly.sql
\set naccounts 100000 * :scale
\setrandom aid 1 :naccounts
SELECT abalance FROM pgbench_accounts WHERE aid = :aid;
sudo -u postgres pgbench -h 192.168.5.180 -p 5432 –c 1 –T 20 –f /var/lib/postgresql/pgbench_script/readonly.sql


# Аристов: если указать не сущ.бд, то pgbouncer умрет. Небходимо включить рестарт сервиса при падении Restart=always
```
###### Админка pgbouncera
```
sudo -u postgres psql -p 6432 pgbouncer -h localhost
show clients;
 type |   user   | database  | state  | addr | port  | local_addr | local_port |      connect_time       |      request_time       | wait | wait_us | close_needed |      ptr       | link | remote_pid | tls
------+----------+-----------+--------+------+-------+------------+------------+-------------------------+-------------------------+------+---------+--------------+----------------+------+------------+-----
 C    | postgres | pgbouncer | active | ::1  | 51976 | ::1        |       6432 | 2022-07-01 13:35:24 MSK | 2022-07-01 13:35:36 MSK |    0 |       0 |            0 | 0x564fa861a780 |      |          0 |

```

##### 5. Тестим на switchover:
```
sudo patronictl -c /etc/patroni.yml switchover
Master [pg2]: - enter
Candidate ['pg1', 'pg3'] []: pg1 - указать кандидата на MASTER
When should the switchover take place (e.g. 2022-07-01T14:38 )  [now]: - enter - начинаем немедленно
Current cluster topology
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Replica | running | 10 |         0 |
| pg2    | 192.168.5.166 | Leader  | running | 10 |           |
| pg3    | 192.168.5.167 | Replica | running | 10 |         0 |
+--------+---------------+---------+---------+----+-----------+
Are you sure you want to switchover cluster patroni, demoting current master pg2? [y/N]: y
2022-07-01 13:39:16.67253 Successfully switched over to "pg1"
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Leader  | running | 10 |           |
| pg2    | 192.168.5.166 | Replica | stopped |    |   unknown |
| pg3    | 192.168.5.167 | Replica | running | 10 |         0 |
+--------+---------------+---------+---------+----+-----------+
+--------+---------------+---------+---------+----+-----------+
| Member | Host          | Role    | State   | TL | Lag in MB |
+ Cluster: patroni (7114613472625263394) ----+----+-----------+
| pg1    | 192.168.5.165 | Leader  | running | 11 |           |
| pg2    | 192.168.5.166 | Replica | running | 11 |         0 |
| pg3    | 192.168.5.167 | Replica | running | 11 |         0 |
+--------+---------------+---------+---------+----+-----------+
```
###### Переключение master с pg2 на pg1 произведено успешно.

### 4. HAPROXY
###### С новых 2-х нод тестим ответ :
```
# Опрос нод PATRONI на критерий master/replica:
curl -v 192.168.5.165:8008/master
curl -v 192.168.5.165:8008/replica
# HTTP/1.0 200 OK
curl -v 192.168.5.166:8008/master
# HTTP/1.0 200 OK
curl -v 192.168.5.167:8008/replica
# HTTP/1.0 200 OK
```

###### Установка HAPROXY
```
# При установке ключа ругается, поэтому устанавливаем:
sudo apt install gnupg -y
sudo apt install gnupg1 -y
sudo apt install gnupg2 -y
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
sudo wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo apt update && sudo apt upgrade -y
sudo apt install postgresql-client-common postgresql-client -y
sudo apt install -y haproxy
sudo vim /etc/haproxy/haproxy.cfg
```
```
listen postgres_write
    bind *:5432
    mode            tcp
    option httpchk
    http-check connect
    http-check send meth GET uri /master
    http-check expect status 200
    default-server inter 10s fall 3 rise 3 on-marked-down shutdown-sessions
    server pg1 192.168.5.165:6432 check port 8008
    server pg2 192.168.5.166:6432 check port 8008
    server pg3 192.168.5.167:6432 check port 8008

listen postgres_read
    bind *:5433
    mode            tcp
    http-check connect
    http-check send meth GET uri /replica
    http-check expect status 200
    default-server inter 10s fall 3 rise 3 on-marked-down shutdown-sessions
    server pg1 192.168.5.165:6432 check port 8008
    server pg2 192.168.5.166:6432 check port 8008
    server pg3 192.168.5.167:6432 check port 8008
    
    # Секция сбора статистики:
frontend stats
    bind *:9000
    stats enable
    stats uri /stats
    stats refresh 10s
    stats auth haproxy:haproxy
```
```
# Проверки:
haproxy -f /etc/haproxy/haproxy.cfg -c
sudo systemctl restart haproxy.service
sudo systemctl status haproxy.service
# Ответ:
июл 05 10:31:44 haproxy1 haproxy[3466]: Proxy postgres_write started.
июл 05 10:31:44 haproxy1 haproxy[3466]: Proxy postgres_read started.
июл 05 10:31:44 haproxy1 systemd[1]: Started HAProxy Load Balancer.
```
```
# Проверим, что все рабочее: c haproxy1 или haproxy2 :
psql -p 6432 -d otus -h 192.168.5.165 -U postgres
create database haproxy;
\q
# Подключение без указания master/replica:
psql -h localhost -d otus -U postgres -p 5432
create database haproxy1;
```
### 5.  KEEPALIVED
###### 1. На haproxy1, haproxy2 установим и вкл. вирт.сетевой адаптер, по умолч. откл:
```
sudo apt install -y keepalived
sudo vim /etc/sysctl.conf
#  вкл. опцию вирт.сетевой адаптер:
net.ipv4.ip_nonlocal_bind=1
# применим внесенные изменения в конфиге:
sudo sysctl -p
# Исправляем в статусе WARNING - default user 'keepalived_script' does not exist - please create.
sudo useradd keepalived_script
# groupadd -r keepalived_script
# useradd -r -s /sbin/nologin -g keepalived_script -M keepalived_script

# смотрим, какие теперь сетевые адаптеры с ip адресами:
ip a
# Ответ haproxy1:
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens18: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether ea:39:5f:71:63:86 brd ff:ff:ff:ff:ff:ff
    altname enp0s18
    inet 192.168.5.168/24 brd 192.168.5.255 scope global noprefixroute ens18
       valid_lft forever preferred_lft forever
    inet 192.168.5.180/24 scope global secondary ens18
       valid_lft forever preferred_lft forever
    inet6 fe80::e839:5fff:fe71:6386/64 scope link noprefixroute
       valid_lft forever preferred_lft forever

# смотрим имя и адрес сетевого интерфейса: ens18
sudo vim /etc/keepalived/keepalived.conf
# Вставляем свой keepalived.conf и keepalived2.conf с уканием virtual_ipadress: 192.168.5.180 с равными id вирт.роутер = 51
# Секция сбора статистики:
frontend stats
    bind *:9000
    stats enable
    stats uri /stats
    stats refresh 10s
    stats auth haproxy:haproxy
```    
```
sudo service keepalived stop
sudo service keepalived start
sudo service keepalived status
```
```
# Ответ haproxy1
июл 05 12:28:58 haproxy1 Keepalived_vrrp[3929]: (VI_01) ip address associated with VRID 51 not present in MASTER advert : 192.168.5.180
июл 05 12:28:59 haproxy1 Keepalived_vrrp[3929]: (VI_01) Received advert from 192.168.5.169 with lower priority 100, ours 111, forcing new election
```
```
# Ответ haproxy2
июл 05 12:28:59 haproxy2 Keepalived_vrrp[1477]: (VI_01) Entering MASTER STATE
июл 05 12:28:59 haproxy2 Keepalived_vrrp[1477]: (VI_01) Master received advert from 192.168.5.168 with higher priority 111, ours 100
июл 05 12:28:59 haproxy2 Keepalived_vrrp[1477]: (VI_01) Entering BACKUP STATE
```
```
apt install nmap
 nmap -v -Pn 192.168.5.180 -p 9000
# Ответ:
PORT     STATE SERVICE
9000/tcp open  cslistener
MAC Address: EA:39:5F:71:63:86 (Unknown)
```
```
psql -p 5432 -d otus -h 192.168.5.180 -U postgres
# Ответ:
Пароль пользователя postgres:
psql (14.4 (Debian 14.4-1.pgdg110+1))
Введите "help", чтобы получить справку.
otus=#
```
```
# Просмотр статистики haproxy:
http://192.168.5.180:9000/stats
```
```
# Просмотр статистики haproxy YANDEX:
http://10.128.0.69:9000/stats
```

```
# Тест patroni через keepalived:
time pgbench -c 1 -j 1 -P 10 -T 300 -h 192.168.5.180 -p 5432 -U postgres otus
# Ответ: tps = 21.353027 real    5m7,465s
# Тест patroni через localhost:
time pgbench -c 1 -j 1 -P 10 -T 300 -h localhost -p 5432 -U postgres otus
# Ответ: tps = 19.153137 real    5m0,293s - вып.комманд с pg1 показал меньший результат
```
# Теста с хоста ZABBIX:
```
# pgbouncer:
time pgbench -c 1 -j 1 -P 10 -T 300 -h 192.168.5.165 -p 6432 -U postgres otus
# Ответ: tps = 20.162557 real    5m10,015s
# patroni:
time pgbench -c 1 -j 1 -P 10 -T 300 -h 192.168.5.165 -p 5432 -U postgres otus
# Ответ: tps = 20.755144 real    5m7,606s
# keepalived:
time pgbench -c 1 -j 1 -P 10 -T 300 -h 192.168.5.180 -p 5432 -U postgres otus
tps = 16.665795 real    5m7,245s

# -c 200 - вываливается с ошибкой, устраняем:
time pgbench -c 200 -j 1 -P 10 -T 300 -h 192.168.5.165 -p 6432 -U postgres otus
SHOW max_connections;
# Правим:
patronictl -c /etc/patroni.yml edit-config
postgresql:
  parameters:
    max_connections: 25
# Постепенно увеличиваю число коннектов:
 sudo -u postgres psql -h localhost
drop database haproxy1;
drop database testpgbench; 
```
##### Самое одекватное при оценке кандидата это ориентимроваться на его проекты и вести дискуссию о сделанных кейсах
##### Т.к. patroni сам управляет, то правим так:
```
ALTER SYSTEM SET shared_buffers = '1GB';
ALTER SYSTEM SET maintenance_work_mem = '256MB';
```
### Команды Patroni
```
ps -aux | grep postgres
pg_ctl stop -D /data/pg_data
pg_ctl start -D /data/pg_data
patronictl list
patronictl restart имя_кластера ip_host
patronictl restart test58 10.126.102.90
patronictl reinit
# Параметр 5 мин:
primary_stat_timeout = 300

```















###### !!! Если поломался старый кластер
```
patronictl -c /etc/patroni.yml remove 7088634863084761990
```
