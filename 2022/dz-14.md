# Горизонтальное масштабирование средствами CITUS v.11.0.
###### 0. Подготовка 4-х нод. 1-я будет координатором.
```
sudo apt update && apt upgrade -y -q
```
```
# Добавляем скрипт от citus data
curl https://install.citusdata.com/community/deb.sh | sudo bash
# The repository is set up! You can now install packages.
```
```
# Уст. сборку postgresql-14 c citus;
apt -y install postgresql-14-citus-10.2
sudo apt-get -y install postgresql-14-citus-11.0
```
###### YANDEX-ноды на внутр.статич.адресах, доменные имена полностью:
```
vim /etc/hosts
10.128.0.30 citus1.ru-central1.internal citus1
10.128.0.31 citus2.ru-central1.internal citus2
10.128.0.32 citus3.ru-central1.internal citus3
10.128.0.33 citus4.ru-central1.internal citus4
10.128.0.34 citus5.ru-central1.internal citus5

10.128.0.40 citus11.ru-central1.internal citus11
10.128.0.41 citus22.ru-central1.internal citus22
10.128.0.42 citus33.ru-central1.internal citus33
10.128.0.43 citus44.ru-central1.internal citus44
10.128.0.44 citus55.ru-central1.internal citus55
```
##### 1. Настройка:

```
# 1.1 Configure connection and authentication:
sudo pg_conftool 14 main set listen_addresses '*'
```
```
# 1.2 - YANDEX- На координаторе в конец файла:
sudo vim /etc/postgresql/14/main/pg_hba.conf
# 1. На координаторе add внутр.подсеть:
host    all             all             10.128.0.0/24          trust
# 2. На координаторе  Вкл вход с localhost по trust:
host    all             all             127.0.0.1/32           trust
# 3. На координаторе  Вход из вне (так не секьюрно):
host    all             all             0.0.0.0/0              scram-sha-256
# 4. На всех нодах trust-аутентификация:
host    all             all             10.128.0.0/24            trust
systemctl restart postgresql
sudo update-rc.d postgresql enable
```

```
# 1.2 -PROXMOX- На координаторе в конец файла:
sudo vim /etc/postgresql/14/main/pg_hba.conf
# 1. add внутр.подсеть:
host  all all 192.168.5.0/24            trust
# 2. Вкл вход с localhost по trust:
host    all             all             127.0.0.1/32         trust
# 3. Вход из вне:
host    all             all             0.0.0.0/0            scram-sha-256
# 4. На всех остальных trust-аутентификация:
host  all all 192.168.5.0/24  trust
systemctl restart postgresql
sudo update-rc.d postgresql enable
sudo -i -u postgres psql
```
```
# 1.3 before adding the first worker node, tell future worker nodes how to reach the coordinator:
SELECT citus_set_coordinator_host('citus1', 5432);
```
```
# 1.4 На каждой ноде preload citus extension: 
vim /etc/postgresql/14/main/postgresql.conf
shared_preload_libraries = 'citus, pg_stat_statements'
sudo pg_conftool 14 main set shared_preload_libraries citus
SHOW shared_preload_libraries;
```
```
# 1.5 На каждой ноде создадим расширение для базовой БД postgresql:
CREATE EXTENSION citus;
sudo -i -u postgres psql -c "CREATE EXTENSION citus;"
```
```
# 1.6 На каждой ноде и координаторе создадим тестовую БД bank:
sudo -i -u postgres psql
create database bank;
```
```
# 1.7 Проверка доступности с координатора:
# v10.2
sudo -i psql -p 5432 -d bank -h 10.128.0.40 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.41 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.42 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.43 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.44 -U postgres
# v11.0
sudo -i psql -p 5432 -d bank -h 10.128.0.30 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.31 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.32 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.33 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.34 -U postgres
# PROXMOX
sudo -i psql -p 5432 -d bank -h 192.168.5.172 -U postgres
sudo -i psql -p 5432 -d bank -h 192.168.5.173 -U postgres
sudo -i psql -p 5432 -d bank -h 192.168.5.174 -U postgres
sudo -i psql -p 5432 -d bank -h 192.168.5.175 -U postgres
```
```
# 1.8 На всех нодах и координаторе создадим расширение для БД bank:
systemctl restart postgresql
sudo -i -u postgres psql
\c bank
CREATE EXTENSION citus;
SELECT * FROM pg_extension;
# Ответ:
  oid  | extname | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition
-------+---------+----------+--------------+----------------+------------+-----------+--------------
 13743 | plpgsql |       10 |           11 | f              | 1.0        |           |
 16384 | citus   |       10 |           11 | f              | 10.2-4     |           |
```
###### 1.9 Подключим ноды citus2 и citus3 на координаторе:
```
\c bank
SELECT * FROM master_add_node('citus2', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus2', 5432);"
# Ответ:
 master_add_node
-----------------
               1
SELECT * FROM master_add_node('citus3', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus3', 5432);"
# Ответ:
 master_add_node
-----------------
               2
# Проверим, что все подключилось:
SELECT * FROM master_get_active_worker_nodes();
sudo -i -u postgres psql -c "SELECT * FROM master_get_active_worker_nodes();"
# Ответ: 
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus2    |      5432
# V11.0 доступна команда:
SELECT * FROM citus_check_cluster_node_health();
 from_nodename | from_nodeport | to_nodename | to_nodeport | result
---------------+---------------+-------------+-------------+--------
 citus2        |          5432 | citus2      |        5432 | f
 citus2        |          5432 | citus3      |        5432 | t
 citus3        |          5432 | citus2      |        5432 | t
 citus3        |          5432 | citus3      |        5432 | f
SELECT * FROM pg_dist_node;
 nodeid | groupid | nodename | nodeport | noderack | hasmetadata | isactive | noderole | nodecluster | metadatasynced | shouldhaveshards
--------+---------+----------+----------+----------+-------------+----------+----------+-------------+----------------+------------------
      1 |       1 | citus2   |     5432 | default  | t           | t        | primary  | default     | t              | t
      2 |       2 | citus3   |     5432 | default  | t           | t        | primary  | default     | t              | t
``` 

######  2. Работа с БД на каждой ноде и координаторе:
```
# 2.1 Создаем тестовую таблицу везде, т.к. это не полноценный кластер:
sudo -i -u postgres psql
\c bank
CREATE TABLE test2(i int);
# После создания таблицы неоюходим ключ шардирования PRIMARY KEY (id), по которому будут разъезжаться данные:
# 2.2 На координаторе:
CREATE TABLE accounts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      balance DECIMAL
  );
# 2.3 Заполним данными:
INSERT INTO accounts (balance)
  VALUES
      (1000.50), (20000), (380), (500), (55000);
# Прверим состояние:
bank=# select * from accounts;
                  id                  | balance
--------------------------------------+---------
 94b52377-aa56-49dd-bf1c-04a8fbf6317a |     380
 2db45df1-168f-4399-9866-1bd59a867dbf |   20000
 a83d6e48-f037-416e-8bf4-4570381bf58b |     500
 33b62bd8-216b-4d8f-8c01-9e666a7a6652 |   55000
 dedba0ca-4b09-4d69-b4ba-fb89278224e9 | 1000.50
(5 rows)
```
```
 # 2.4 !!! Шардируем. В самом конце на координаторе:
# Проверим и установим фактор репликации:
\c bank
show citus.shard_replication_factor;
alter database bank set citus.shard_replication_factor = '2';
show citus.shard_replication_factor;
# V11.0:
SELECT * FROM citus_check_cluster_node_health();
# Разъезжаемся:
SELECT create_distributed_table('accounts', 'id');
```
```
# Проверка на ноде 2:
bank=# \dt
          List of relations
 Schema |   Name   | Type  |  Owner
--------+----------+-------+----------
 public | accounts | table | postgres
 public | test2    | table | postgres
(2 rows)
```
```
# На координаторе, как понять где, что разъехалось:
SELECT * FROM pg_dist_shard;
(32 rows)
```
###### 3. Добавляем еще одну ноду citus4:
```
# 3.1  Настройка citus4
# Правим конфиг:
# Создаем расширениедля postgres:
# Создаем базу bank:
# Создаем расширение для bank:
```
```
# 3.2 На координаторе подключаем ноду citus4:
# Не работает:
SELECT * FROM master_add_node('10.128.33', 5432);
# Работает:
\c bank
SELECT * FROM master_add_node('citus4', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus4', 5432);"
 master_add_node
-----------------
               4
(1 row)
# Проверим добавление:
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus4    |      5432
 citus2    |      5432
(3 rows)
```
```
# Проверка содержимого БД:
\c bank
\dt+
```
###### 4. После добавления или удаления необходим ребаланс нод:
```
# Ребаланс нод на координаторе:
sudo -i -u postgres psql
\c bank
bank=# SELECT rebalance_table_shards('accounts');
ОШИБКА: connection to the remote node localhost:5432 failed with the following error: fe_sendauth: no password supplied
# Исправляем ошибку:
vim /var/lib/postgresql/.pgpass
localhost:5432:bank:postgres:12345
sudo chmod 600 /var/lib/postgresql/.pgpass
sudo chown postgres:postgres /var/lib/postgresql/.pgpass
ls -la /var/lib/postgresql
ALTER ROLE postgres WITH PASSWORD '12345';

# Проверка доступности с координатора:
sudo -i psql -p 5432 -d bank -h 10.128.0.33 -U postgres   
sudo -i psql -p 5432 -d bank -h 192.168.5.175 -U postgres
SELECT rebalance_table_shards('accounts');
NOTICE:  Moving shard 102008 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102009 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102010 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102011 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102012 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102013 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102014 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102015 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102016 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102017 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102018 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102019 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102020 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102021 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102022 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102023 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102024 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102025 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102026 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102027 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102028 from citus33:5432 to citus44:5432 ...
 rebalance_table_shards
------------------------
(1 row)
```
```
# Согласно документации: Включить логическую репликацию в исходной базе данных:
vim /etc/postgresql/14/main/postgresql.conf
wal_level = logical
max_replication_slots = 5
max_wal_senders = 5 
```
```
show wal_level;
postgres=# ALTER SYSTEM SET wal_level = logical;
```

###### Создание ПУБЛИКАЦИИ на координаторе citus1
```
CREATE PUBLICATION accounts_pub FOR TABLE accounts;
\dRp+
Публикация accounts_pub
 Владелец | Все таблицы | Добавления | Изменения | Удаления | Опустошения | Через корень
----------+-------------+------------+-----------+----------+-------------+--------------
 postgres | f           | t          | t         | t        | t           | f
Таблицы:
    "public.accounts"
```
###### Cоздание ПОДПИСКИ на citus3 без переноса начальных данных с CITUS1:
```
# БД и таблица accounts должны существовать:
CREATE SUBSCRIPTION accounts_sub
CONNECTION 'host=10.128.0.30 port=5432 user=postgres password=12345 dbname=bank' 
PUBLICATION accounts_pub WITH (copy_data = false);


CREATE SUBSCRIPTION accounts_sub
CONNECTION 'host=192.168.5.172 port=5432 user=postgres password=12345 dbname=bank' 
PUBLICATION accounts_pub WITH (copy_data = false);
# copy_data = false - создание подписки без переноса исходных данных.
```
###### РЕБАЛАНС
```
bank=# SELECT rebalance_table_shards('accounts');
NOTICE:  Moving shard 102008 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102009 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102010 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102011 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102012 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102013 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102014 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102015 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102016 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102017 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102018 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102019 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102020 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102021 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102022 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102023 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102024 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102025 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102026 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102027 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102028 from citus3:5432 to citus4:5432 ...
 rebalance_table_shards
------------------------
(1 row)
```
##### На координаторе добавляем ноду citus5:
```
\c bank
SELECT * FROM master_add_node('citus5', 5432);
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus5    |      5432
 citus4    |      5432
 citus2    |      5432
(4 rows)
# Проверим:
SELECT * FROM master_get_active_worker_nodes();
bank=# SELECT * FROM citus_check_cluster_node_health();
 from_nodename | from_nodeport | to_nodename | to_nodeport | result
---------------+---------------+-------------+-------------+--------
 citus2        |          5432 | citus2      |        5432 | f
 citus2        |          5432 | citus3      |        5432 | t
 citus2        |          5432 | citus4      |        5432 | t
 citus2        |          5432 | citus5      |        5432 | t
 citus3        |          5432 | citus2      |        5432 | t
 citus3        |          5432 | citus3      |        5432 | f
 citus3        |          5432 | citus4      |        5432 | t
 citus3        |          5432 | citus5      |        5432 | t
 citus4        |          5432 | citus2      |        5432 | t
 citus4        |          5432 | citus3      |        5432 | t
 citus4        |          5432 | citus4      |        5432 | f
 citus4        |          5432 | citus5      |        5432 | t
 citus5        |          5432 | citus2      |        5432 | t
 citus5        |          5432 | citus3      |        5432 | t
 citus5        |          5432 | citus4      |        5432 | t
 citus5        |          5432 | citus5      |        5432 | f
(16 rows)
```
###### На citus5
```
 \c bank
bank=# ALTER ROLE postgres WITH PASSWORD '12345';
ERROR:  operation is not allowed on this node
HINT:  Connect to the coordinator and run it again.
show citus.shard_replication_factor;
 citus.shard_replication_factor
--------------------------------
 2
 \c bank
CREATE TABLE test2(i int);
bank=# CREATE SUBSCRIPTION accounts_sub
CONNECTION 'host=10.128.0.30 port=5432 user=postgres password=12345 dbname=bank'
PUBLICATION accounts_pub WITH (copy_data = false);
ERROR:  could not create replication slot "accounts_sub": ERROR:  replication slot "accounts_sub" already exists
```
##### На координаторе РЕБАЛАНС:
```
bank=# SELECT rebalance_table_shards('accounts');
NOTICE:  Moving shard 102008 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102009 from citus4:5432 to citus5:5432 ...
NOTICE:  Moving shard 102011 from citus3:5432 to citus5:5432 ...
NOTICE:  Moving shard 102010 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102012 from citus4:5432 to citus5:5432 ...
NOTICE:  Moving shard 102013 from citus3:5432 to citus5:5432 ...
NOTICE:  Moving shard 102014 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102015 from citus4:5432 to citus5:5432 ...
NOTICE:  Moving shard 102017 from citus3:5432 to citus5:5432 ...
NOTICE:  Moving shard 102016 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102018 from citus4:5432 to citus5:5432 ...
NOTICE:  Moving shard 102019 from citus3:5432 to citus5:5432 ...
NOTICE:  Moving shard 102020 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102021 from citus4:5432 to citus5:5432 ...
NOTICE:  Moving shard 102023 from citus3:5432 to citus5:5432 ...
NOTICE:  Moving shard 102022 from citus2:5432 to citus5:5432 ...
 rebalance_table_shards
------------------------
Time: 96768.789 ms (01:36.769)
```
#### Удаление citus2 через Draine:
```
# Забираем данные с ноды:
# Не работает:
bank=# SELECT * from citus_drain_node('10.128.0.31', 5432);
ERROR:  node at "10.128.0.31:5432" does not exist
CONTEXT:  while executing command on localhost:5432
Time: 16.611 ms
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus5    |      5432
 citus4    |      5432
 citus2    |      5432
(4 rows)
Time: 0.691 ms

# Так сработало:
bank=# SELECT * FROM citus_drain_node('citus2', 5432);
NOTICE:  Moving shard 102012 from citus2:5432 to citus3:5432 ...
NOTICE:  Moving shard 102018 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102024 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102026 from citus2:5432 to citus3:5432 ...
NOTICE:  Moving shard 102028 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102029 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102030 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102031 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102032 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102033 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102034 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102035 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102036 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102037 from citus2:5432 to citus5:5432 ...
NOTICE:  Moving shard 102038 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102039 from citus2:5432 to citus5:5432 ...
 citus_drain_node
------------------
(1 row)
Time: 50931.579 ms (00:50.932)
bank=#
```
```
# Не использовал команду:
SELECT citus_disable_node('citus2', 5432);
# использовал:
bank=# SELECT * FROM pg_dist_shard;
bank=# SELECT master_remove_node('citus2', 5432);
 master_remove_node
--------------------
(1 row)
# Проверка состояния нод:
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus5    |      5432
 citus4    |      5432
(3 rows)
SELECT * from pg_dist_node;
 nodeid | groupid | nodename | nodeport | noderack | hasmetadata | isactive | noderole | nodecluster | metadatasynced | shouldhaveshards
--------+---------+----------+----------+----------+-------------+----------+----------+-------------+----------------+------------------
      2 |       2 | citus3   |     5432 | default  | t           | t        | primary  | default     | t              | t
      3 |       3 | citus4   |     5432 | default  | t           | t        | primary  | default     | t              | t
      4 |       4 | citus5   |     5432 | default  | t           | t        | primary  | default     | t              | t
(3 rows)
```
```
bank=# SELECT rebalance_table_shards('accounts');
NOTICE:  Moving shard 102008 from citus5:5432 to citus3:5432 ...
NOTICE:  Moving shard 102010 from citus4:5432 to citus3:5432 ...
NOTICE:  Moving shard 102011 from citus5:5432 to citus3:5432 ...
 rebalance_table_shards
------------------------
(1 row)
```
```
SELECT * from pg_dist_shard;
(32 rows)
SELECT * FROM citus_shards;
(64 rows)
```
#####  Сводка всех таблиц, управляемых Citus (распределенные и справочные таблицы) на координаторе:
```
bank=# SELECT * FROM citus_tables;
 table_name | citus_table_type | distribution_column | colocation_id | table_size | shard_count | table_owner | access_method
------------+------------------+---------------------+---------------+------------+-------------+-------------+---------------
 accounts   | distributed      | id                  |             1 | 1184 kB    |          32 | postgres    | heap
(1 row)
bank=# SELECT * FROM pg_dist_rebalance_strategy;
      name      | default_strategy |      shard_cost_function      | node_capacity_function |  shard_allowed_on_node_function  | default_threshold | minimum_threshold | improvement_threshold
----------------+------------------+-------------------------------+------------------------+----------------------------------+-------------------+-------------------+-----------------------
 by_shard_count | t                | citus_shard_cost_1            | citus_node_capacity_1  | citus_shard_allowed_on_node_true |                 0 |                 0 |                     0
 by_disk_size   | f                | citus_shard_cost_by_disk_size | citus_node_capacity_1  | citus_shard_allowed_on_node_true |               0.1 |              0.01 |                   0.5
(2 rows)

postgres=# create extension pg_stat_statements;
CREATE EXTENSION citus;

bank=# select * from citus_stat_statements;
 queryid | userid | dbid | query | executor | partition_key | calls
---------+--------+------+-------+----------+---------------+-------
(0 rows)

SELECT * FROM pg_stat_statements \gx
-[ RECORD 1 ]-------+---------------------------------
userid              | 10
dbid                | 13757
toplevel            | t
queryid             | -1310408793821239024
query               | SELECT * FROM pg_stat_statements
plans               | 0
total_plan_time     | 0
min_plan_time       | 0
max_plan_time       | 0
mean_plan_time      | 0
stddev_plan_time    | 0
calls               | 2
total_exec_time     | 0.190834
min_exec_time       | 0.093264
max_exec_time       | 0.09757
mean_exec_time      | 0.095417
stddev_exec_time    | 0.002153000000000002
rows                | 1
shared_blks_hit     | 0
shared_blks_read    | 0
shared_blks_dirtied | 0
shared_blks_written | 0
local_blks_hit      | 0
local_blks_read     | 0
local_blks_dirtied  | 0
local_blks_written  | 0
temp_blks_read      | 0
temp_blks_written   | 0
blk_read_time       | 0
blk_write_time      | 0
wal_records         | 0
wal_fpi             | 0
wal_bytes           | 0
```















