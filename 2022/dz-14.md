# Горизонтальное масштабирование средствами CITUS.
###### 0. Подготовка 4-х нод. 1-я будет координатором.
```
sudo apt update && apt upgrade -y -q
```
```
# Добавляем скрипт от citus data
curl https://install.citusdata.com/community/deb.sh | sudo bash
# The repository is set up! You can now install packages.
```
```
# Уст. сборку postgresql-14 c citus;
apt -y install postgresql-14-citus-10.2
```
###### YANDEX-ноды на внутр.статич.адресах, доменные имена полностью:
```
vim /etc/hosts
10.128.0.30 citus1.ru-central1.internal citus1
10.128.0.31 citus2.ru-central1.internal citus2
10.128.0.32 citus3.ru-central1.internal citus3
10.128.0.33 citus4.ru-central1.internal citus4
10.128.0.34 citus5.ru-central1.internal citus5

10.128.0.40 citus11.ru-central1.internal citus11
10.128.0.41 citus22.ru-central1.internal citus22
10.128.0.42 citus33.ru-central1.internal citus33
10.128.0.43 citus44.ru-central1.internal citus44
```
##### 1. Настройка:

```
# 1.1 Configure connection and authentication:
sudo pg_conftool 14 main set listen_addresses '*'
```
```
# 1.2 - YANDEX- На координаторе в конец файла:
sudo vim /etc/postgresql/14/main/pg_hba.conf
# 1. add внутр.подсеть:
host    all             all             10.128.0.0/24          trust
# 2. Вкл вход с localhost по trust:
host    all             all             127.0.0.1/32           trust
# 3. Вход из вне (так не секьюрно):
host    all             all             0.0.0.0/0              scram-sha-256
# 4. На всех нодах trust-аутентификация:
host    all             all             10.128.0.0/24            trust
systemctl restart postgresql
sudo update-rc.d postgresql enable
```

```
# 1.2 -PROXMOX- На координаторе в конец файла:
sudo vim /etc/postgresql/14/main/pg_hba.conf
# 1. add внутр.подсеть:
host  all all 192.168.5.0/24            trust
# 2. Вкл вход с localhost по trust:
host    all             all             127.0.0.1/32         trust
# 3. Вход из вне:
host    all             all             0.0.0.0/0            scram-sha-256
# 4. На всех остальных trust-аутентификация:
host  all all 192.168.5.0/24  trust
systemctl restart postgresql
sudo update-rc.d postgresql enable
```
```
# 1.3 Прверка работоспособности после изминений pg_hba.conf:
sudo -i -u postgres psql
```
```
# 1.4 На каждой ноде preload citus extension: 
sudo pg_conftool 14 main set shared_preload_libraries citus
```
```
# 1.5 На каждой ноде создадим расширение для базовой БД postgresql:
CREATE EXTENSION citus;
sudo -i -u postgres psql -c "CREATE EXTENSION citus;"
```

```
# 1.6 На каждой ноде и координаторе создадим тестовую БД bank:
sudo -i -u postgres psql
create database bank;
```
```
# 1.7 На всех нодах и координаторе создадим расширение для БД bank:
systemctl restart postgresql
sudo -i -u postgres psql
\c bank
CREATE EXTENSION citus;
SELECT * FROM pg_extension;
# Ответ:
  oid  | extname | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition
-------+---------+----------+--------------+----------------+------------+-----------+--------------
 13743 | plpgsql |       10 |           11 | f              | 1.0        |           |
 16384 | citus   |       10 |           11 | f              | 10.2-4     |           |
```
###### 1.8 Подключим 2 ноды citus2 и citus3 на координаторе:
```
\c bank
SELECT * FROM master_add_node('citus2', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus2', 5432);"
# Ответ:
 master_add_node
-----------------
               1
SELECT * FROM master_add_node('citus3', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus3', 5432);"
# Ответ:
 master_add_node
-----------------
               2
# Проверим, что все подключилось:
SELECT * FROM master_get_active_worker_nodes();
sudo -i -u postgres psql -c "SELECT * FROM master_get_active_worker_nodes();"
# Ответ: 
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus2    |      5432
``` 

######  2. Работа с БД на каждой ноде и координаторе:
```
# 2.1 Создаем тестовую таблицу везде, т.к. это не полноценный кластер:
sudo -i -u postgres psql
\c bank
CREATE TABLE test2(i int);
# После создания таблицы неоюходим ключ шардирования PRIMARY KEY (id), по которому данные будут разъезжаться по нодам данные:
# 2.2 На координаторе:
CREATE TABLE accounts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      balance DECIMAL
  );
# 2.3 Заполним данными:
INSERT INTO accounts (balance)
  VALUES
      (1000.50), (20000), (380), (500), (55000);
# Прверим состояние:
select * from accounts;
                  id                  | balance
--------------------------------------+---------
 841ebf58-519c-4c84-b6dc-2d7b4e578137 | 1000.50
 371a49e9-0e08-4647-82f8-0fd0e278141b |   20000
 3c82e8df-3c07-4d42-9a67-5e677a7668b2 |     380
 4ffde15a-1bbd-4ce5-b631-c5645ee6ccd8 |     500
 f7d6cdc6-3b24-41c1-bdbe-04379225bde9 |   55000
```
```
 # 2.4 !!! Шардируем. В самом конце на координаторе:
# Проверим и установим фактор репликации:
\c bank
show citus.shard_replication_factor;
alter database bank set citus.shard_replication_factor = '2';
show citus.shard_replication_factor;
# Разъезжаемся:
SELECT create_distributed_table('accounts', 'id');
NOTICE:  Copying data from local table...
NOTICE:  copying the data has completed
DETAIL:  The local data in the table is no longer visible, but is still on disk.
HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.accounts$$)
 create_distributed_table
--------------------------
(1 row)

```
```
# Проверка на ноде 2, где citus.shard_replication_factor = '1':
\dt
 public | accounts_102020 | table | postgres
 public | accounts_102022 | table | postgres
 public | accounts_102024 | table | postgres
 public | accounts_102026 | table | postgres
 public | accounts_102028 | table | postgres
 public | accounts_102030 | table | postgres
 public | accounts_102032 | table | postgres
 public | accounts_102034 | table | postgres
 public | accounts_102036 | table | postgres
 public | accounts_102038 | table | postgres
 public | test2           | table | postgres
(17 rows)
# citus.shard_replication_factor = '2'
(33 rows)
```
```
# Проверка на ноде 3  где citus.shard_replication_factor = '1':
\dt
 public | accounts_102020 | table | postgres
 public | accounts_102022 | table | postgres
 public | accounts_102024 | table | postgres
 public | accounts_102026 | table | postgres
 public | accounts_102028 | table | postgres
 public | accounts_102030 | table | postgres
 public | accounts_102032 | table | postgres
 public | accounts_102034 | table | postgres
 public | accounts_102036 | table | postgres
 public | accounts_102038 | table | postgres
 public | test2           | table | postgres
(17 rows)
# citus.shard_replication_factor = '2'
(33 rows)
```
```
# Проверка на ноде 4:
\dt
 Schema | Name  | Type  |  Owner
--------+-------+-------+----------
 public | test2 | table | postgres
(1 row)
```
```
# На координаторе, как понять где, что разъехалось???:
SELECT * FROM pg_dist_shard;
# Ответ:
(32 rows)
```

###### 3. Добавляем еще одну ноду citus4:
```
# 3.1  Настройка citus4
# Правим конфиг:
# Создаем расширениедля postgres:
# Создаем базу bank:
# Создаем расширение для bank:
```
```
# 3.2 На координаторе подключаем ноду citus4:
# Работает:
\c bank
SELECT * FROM master_add_node('citus4', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus4', 5432);"
 master_add_node
-----------------
               4
(1 row)
# Проверим добавление:
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus4    |      5432
 citus2    |      5432
(3 rows)
```

```
# Проверка содержимого БД:
\c bank
\dt+
```
```
# Если убирать ноду, к примеру citus4:
SELECT master_remove_node('citus4', 5432);
 master_remove_node
--------------------
(1 строка)
SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus2    |      5432
(2 строки)
````
###### 4. После добавления или удаления необходим ребаланс нод:
```
# ребаланс нод на координаторе:
sudo -i -u postgres psql
\c bank
bank=# SELECT rebalance_table_shards('accounts');
ERROR:  connection to the remote node localhost:5432 failed with the following error: fe_sendauth: no password supplied
ОШИБКА: connection to the remote node localhost:5432 failed with the following error: fe_sendauth: no password supplied
# Аристов:
vim /var/lib/postgresql/.pgpass
localhost:5432:bank:postgres:12345
sudo chmod 600 /var/lib/postgresql/.pgpass
sudo chown postgres:postgres /var/lib/postgresql/.pgpass
ls -la /var/lib/postgresql


# Задаем пароль:
bank=# \password
Enter new password for user "postgres":
Enter it again:
# Проверка доступности с координатора:
sudo -i psql -p 5432 -d bank -h 10.128.0.33 -U postgres   
sudo -i psql -p 5432 -d bank -h 192.168.5.175 -U postgres
SELECT rebalance_table_shards('accounts');
```
###### 5. Добавляем еще одну ноду citus5:
```
# 5.1  Настройка citus5
sudo apt update && apt upgrade -y -q
curl https://install.citusdata.com/community/deb.sh | sudo bash
apt -y install postgresql-14-citus-10.2
# Правим конфиг:
# Создаем расширениедля postgres:
sudo -i -u postgres psql
CREATE EXTENSION citus;
# Создаем базу bank:
sudo -i -u postgres psql
create database bank;
\c bank
# Создаем расширение для bank:
CREATE EXTENSION citus;
CREATE TABLE test2(i int);
```
```
# 5.2 На координаторе подключаем ноду citus5:
# Проверяяем доступность:
sudo -i psql -p 5432 -d bank -h 10.128.0.34 -U postgres
# Работает:
\c bank
SELECT * FROM master_add_node('citus5', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus5', 5432);"
 master_add_node
-----------------
               4
(1 row)
# Проверим добавление:
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus4    |      5432
 citus2    |      5432
(3 rows)
```


###### Проверка степени отказоустойчивости:
```
sudo -i -u postgres psql -c "show citus.shard_replication_factor;"
 citus.shard_replication_factor
--------------------------------
 1
 # 1 означает, что при падении 1 воркера все теряется, кроме координатора 
```

```
# убрать ноду:
SELECT master_remove_node('citus1', 5432);
```
```
CREATE TABLE test (
    Region VARCHAR(50),
    Country VARCHAR(50),
    ItemType VARCHAR(50),
    SalesChannel VARCHAR(20),
    OrderPriority VARCHAR(10),
    OrderDate VARCHAR(10),
    OrderID int,
    ShipDate VARCHAR(10),
    UnitsSold int,
    UnitPrice decimal(12,2),
    UnitCost decimal(12,2),
    TotalRevenue decimal(12,2),
    TotalCost decimal(12,2),
    TotalProfit decimal(12,2)
);
ALTER TABLE test ADD PRIMARY KEY (OrderID);
SELECT create_distributed_table('test', 'orderid');


```















