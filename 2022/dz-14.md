# Горизонтальное масштабирование средствами CITUS.
###### 0. Подготовка 4-х нод. 1-я будет координатором.
```
sudo apt update && apt upgrade -y -q
```
```
# Добавляем скрипт от citus data
curl https://install.citusdata.com/community/deb.sh | sudo bash
# The repository is set up! You can now install packages.
```
```
# Уст. сборку postgresql-14 c citus;
apt -y install postgresql-14-citus-10.2
sudo apt-get -y install postgresql-14-citus-11.0
```
###### YANDEX-ноды на внутр.статич.адресах, доменные имена полностью:
```
vim /etc/hosts
10.128.0.30 citus1.ru-central1.internal citus1
10.128.0.31 citus2.ru-central1.internal citus2
10.128.0.32 citus3.ru-central1.internal citus3
10.128.0.33 citus4.ru-central1.internal citus4
10.128.0.34 citus5.ru-central1.internal citus5

10.128.0.40 citus11.ru-central1.internal citus11
10.128.0.41 citus22.ru-central1.internal citus22
10.128.0.42 citus33.ru-central1.internal citus33
10.128.0.43 citus44.ru-central1.internal citus44
10.128.0.44 citus55.ru-central1.internal citus55
```
##### 1. Настройка:

```
# 1.1 Configure connection and authentication:
sudo pg_conftool 14 main set listen_addresses '*'
```
```
# 1.2 - YANDEX- На координаторе в конец файла:
sudo vim /etc/postgresql/14/main/pg_hba.conf
# 1. На координаторе add внутр.подсеть:
host    all             all             10.128.0.0/24          trust
# 2. На координаторе  Вкл вход с localhost по trust:
host    all             all             127.0.0.1/32           trust
# 3. На координаторе  Вход из вне (так не секьюрно):
host    all             all             0.0.0.0/0              scram-sha-256
# 4. На всех нодах trust-аутентификация:
host    all             all             10.128.0.0/24            trust
systemctl restart postgresql
sudo update-rc.d postgresql enable
```

```
# 1.2 -PROXMOX- На координаторе в конец файла:
sudo vim /etc/postgresql/14/main/pg_hba.conf
# 1. add внутр.подсеть:
host  all all 192.168.5.0/24            trust
# 2. Вкл вход с localhost по trust:
host    all             all             127.0.0.1/32         trust
# 3. Вход из вне:
host    all             all             0.0.0.0/0            scram-sha-256
# 4. На всех остальных trust-аутентификация:
host  all all 192.168.5.0/24  trust
systemctl restart postgresql
sudo update-rc.d postgresql enable
```
```
# 1.3 Прверка работоспособности после изминений pg_hba.conf:
sudo -i -u postgres psql
```
```
# 1.4 На каждой ноде preload citus extension: 
sudo pg_conftool 14 main set shared_preload_libraries citus
```
```
# 1.5 На каждой ноде создадим расширение для базовой БД postgresql:
CREATE EXTENSION citus;
sudo -i -u postgres psql -c "CREATE EXTENSION citus;"
```

```
# 1.6 На каждой ноде и координаторе создадим тестовую БД bank:
sudo -i -u postgres psql
create database bank;
```
```
# 1.7 Проверка доступности с координатора:
# v10.2
sudo -i psql -p 5432 -d bank -h 10.128.0.40 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.41 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.42 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.43 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.44 -U postgres
# v11.0
sudo -i psql -p 5432 -d bank -h 10.128.0.30 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.31 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.32 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.33 -U postgres
sudo -i psql -p 5432 -d bank -h 10.128.0.34 -U postgres


sudo -i psql -p 5432 -d bank -h 192.168.5.172 -U postgres
sudo -i psql -p 5432 -d bank -h 192.168.5.173 -U postgres
sudo -i psql -p 5432 -d bank -h 192.168.5.174 -U postgres
sudo -i psql -p 5432 -d bank -h 192.168.5.175 -U postgres
```
```
# 1.8 На всех нодах и координаторе создадим расширение для БД bank:
systemctl restart postgresql
sudo -i -u postgres psql
\c bank
CREATE EXTENSION citus;
SELECT * FROM pg_extension;
# Ответ:
  oid  | extname | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition
-------+---------+----------+--------------+----------------+------------+-----------+--------------
 13743 | plpgsql |       10 |           11 | f              | 1.0        |           |
 16384 | citus   |       10 |           11 | f              | 10.2-4     |           |
```
###### 1.9 Подключим 2 ноды citus2 и citus3 на координаторе:
```
\c bank
SELECT * FROM master_add_node('citus2', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus2', 5432);"
# Ответ:
 master_add_node
-----------------
               1
SELECT * FROM master_add_node('citus3', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus3', 5432);"
# Ответ:
 master_add_node
-----------------
               2
# Проверим, что все подключилось:
SELECT * FROM master_get_active_worker_nodes();
sudo -i -u postgres psql -c "SELECT * FROM master_get_active_worker_nodes();"
# Ответ: 
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus2    |      5432
# V11.0 доступна команда:
SELECT * FROM citus_check_cluster_node_health();
 from_nodename | from_nodeport | to_nodename | to_nodeport | result
---------------+---------------+-------------+-------------+--------
 citus2        |          5432 | citus2      |        5432 | f
 citus2        |          5432 | citus3      |        5432 | t
 citus3        |          5432 | citus2      |        5432 | t
 citus3        |          5432 | citus3      |        5432 | f
SELECT * FROM pg_dist_node;
 nodeid | groupid | nodename | nodeport | noderack | hasmetadata | isactive | noderole | nodecluster | metadatasynced | shouldhaveshards
--------+---------+----------+----------+----------+-------------+----------+----------+-------------+----------------+------------------
      1 |       1 | citus2   |     5432 | default  | t           | t        | primary  | default     | t              | t
      2 |       2 | citus3   |     5432 | default  | t           | t        | primary  | default     | t              | t
``` 

######  2. Работа с БД на каждой ноде и координаторе:
```
# 2.1 Создаем тестовую таблицу везде, т.к. это не полноценный кластер:
sudo -i -u postgres psql
\c bank
CREATE TABLE test2(i int);
# После создания таблицы неоюходим ключ шардирования PRIMARY KEY (id), по которому данные будут разъезжаться по нодам данные:
# 2.2 На координаторе:
CREATE TABLE accounts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      balance DECIMAL
  );
# 2.3 Заполним данными:
INSERT INTO accounts (balance)
  VALUES
      (1000.50), (20000), (380), (500), (55000);
# Прверим состояние:
bank=# select * from accounts;
                  id                  | balance
--------------------------------------+---------
 94b52377-aa56-49dd-bf1c-04a8fbf6317a |     380
 2db45df1-168f-4399-9866-1bd59a867dbf |   20000
 a83d6e48-f037-416e-8bf4-4570381bf58b |     500
 33b62bd8-216b-4d8f-8c01-9e666a7a6652 |   55000
 dedba0ca-4b09-4d69-b4ba-fb89278224e9 | 1000.50
(5 rows)
```
```
 # 2.4 !!! Шардируем. В самом конце на координаторе:
# Проверим и установим фактор репликации:
\c bank
show citus.shard_replication_factor;
alter database bank set citus.shard_replication_factor = '2';
show citus.shard_replication_factor;
# V11.0:
SELECT * FROM citus_check_cluster_node_health();
# Разъезжаемся:
SELECT create_distributed_table('accounts', 'id');
# Аристов изменил:
SELECT create_distributed_table('accounts3', 'id');
NOTICE:  Copying data from local table...
NOTICE:  copying the data has completed
DETAIL:  The local data in the table is no longer visible, but is still on disk.
HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.accounts$$)
 create_distributed_table
--------------------------
(1 row)
# Если повторно, то:
ERROR:  table "accounts3" is already distributed
```
```
# Проверка на ноде 2, где citus.shard_replication_factor = '1':
\dt
 public | accounts_102020 | table | postgres
 public | accounts_102022 | table | postgres
 public | accounts_102024 | table | postgres
 public | accounts_102026 | table | postgres
 public | accounts_102028 | table | postgres
 public | accounts_102030 | table | postgres
 public | accounts_102032 | table | postgres
 public | accounts_102034 | table | postgres
 public | accounts_102036 | table | postgres
 public | accounts_102038 | table | postgres
 public | test2           | table | postgres
(17 rows)
# citus.shard_replication_factor = '2'
(33 rows)
```
```
# Проверка на ноде 3, где citus.shard_replication_factor = '1':
\dt
 public | accounts_102020 | table | postgres
 public | accounts_102022 | table | postgres
 public | accounts_102024 | table | postgres
 public | accounts_102026 | table | postgres
 public | accounts_102028 | table | postgres
 public | accounts_102030 | table | postgres
 public | accounts_102032 | table | postgres
 public | accounts_102034 | table | postgres
 public | accounts_102036 | table | postgres
 public | accounts_102038 | table | postgres
 public | test2           | table | postgres
(17 rows)
# citus.shard_replication_factor = '2'
(33 rows)
```
```
# Проверка на ноде 4:
\dt
 Schema | Name  | Type  |  Owner
--------+-------+-------+----------
 public | test2 | table | postgres
(1 row)
```
```
# На координаторе, как понять где, что разъехалось:
SELECT * FROM pg_dist_shard;
# Ответ citus.shard_replication_factor = '1' или '2' совпадает:
(32 rows)
```

###### 3. Добавляем еще одну ноду citus4:
```
# 3.1  Настройка citus4
# Правим конфиг:
# Создаем расширениедля postgres:
# Создаем базу bank:
# Создаем расширение для bank:
```
```
# 3.2 На координаторе подключаем ноду citus4:
# Работает:
\c bank
SELECT * FROM master_add_node('citus4', 5432);
sudo -i -u postgres psql -c "SELECT * FROM master_add_node('citus4', 5432);"
 master_add_node
-----------------
               4
(1 row)
# Проверим добавление:
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus4    |      5432
 citus2    |      5432
(3 rows)
```

```
# Проверка содержимого БД:
\c bank
\dt+
```
```
# Если убирать ноду, к примеру citus4:
SELECT master_remove_node('citus4', 5432);
 master_remove_node
--------------------
(1 строка)
SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus3    |      5432
 citus2    |      5432
(2 строки)
````
###### 4. После добавления или удаления необходим ребаланс нод:
```
# ребаланс нод на координаторе citus11:
sudo -i -u postgres psql
\c bank
bank=# SELECT rebalance_table_shards('accounts');
ERROR:  connection to the remote node localhost:5432 failed with the following error: fe_sendauth: no password supplied
ОШИБКА: connection to the remote node localhost:5432 failed with the following error: fe_sendauth: no password supplied
# Аристов:
vim /var/lib/postgresql/.pgpass
localhost:5432:bank:postgres:12345
sudo chmod 600 /var/lib/postgresql/.pgpass
sudo chown postgres:postgres /var/lib/postgresql/.pgpass
ls -la /var/lib/postgresql
ALTER ROLE postgres WITH PASSWORD '12345';


# Задаем пароль:
bank=# \password
Enter new password for user "postgres":
Enter it again:
# Проверка доступности с координатора:
sudo -i psql -p 5432 -d bank -h 10.128.0.33 -U postgres   
sudo -i psql -p 5432 -d bank -h 192.168.5.175 -U postgres
SELECT rebalance_table_shards('accounts');
NOTICE:  Moving shard 102008 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102009 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102010 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102011 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102012 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102013 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102014 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102015 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102016 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102017 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102018 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102019 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102020 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102021 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102022 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102023 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102024 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102025 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102026 from citus33:5432 to citus44:5432 ...
NOTICE:  Moving shard 102027 from citus22:5432 to citus44:5432 ...
NOTICE:  Moving shard 102028 from citus33:5432 to citus44:5432 ...
 rebalance_table_shards
------------------------
(1 row)
```
```
# Проверка на ноде 22, citus.shard_replication_factor = '2':
bank=# \dt+
                                          List of relations
 Schema |      Name       | Type  |  Owner   | Persistence | Access method |    Size    | Description
--------+-----------------+-------+----------+-------------+---------------+------------+-------------
 public | accounts_102008 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102010 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102012 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102014 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102016 | table | postgres | permanent   | heap          | 16 kB      |
 public | accounts_102018 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102020 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102022 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102024 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102026 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102028 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102029 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102030 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102031 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102032 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102033 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102034 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102035 | table | postgres | permanent   | heap          | 16 kB      |
 public | accounts_102036 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102037 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102038 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102039 | table | postgres | permanent   | heap          | 8192 bytes |
 public | test2           | table | postgres | permanent   | heap          | 0 bytes    |
(23 rows)
```
```
# Проверка на ноде 33, citus.shard_replication_factor = '2':
bank=# \dt+
                                          List of relations
 Schema |      Name       | Type  |  Owner   | Persistence | Access method |    Size    | Description
--------+-----------------+-------+----------+-------------+---------------+------------+-------------
 public | accounts_102009 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102011 | table | postgres | permanent   | heap          | 16 kB      |
 public | accounts_102013 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102015 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102017 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102019 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102021 | table | postgres | permanent   | heap          | 16 kB      |
 public | accounts_102023 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102025 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102027 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102029 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102030 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102031 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102032 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102033 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102034 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102035 | table | postgres | permanent   | heap          | 16 kB      |
 public | accounts_102036 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102037 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102038 | table | postgres | permanent   | heap          | 8192 bytes |
 public | accounts_102039 | table | postgres | permanent   | heap          | 8192 bytes |
 public | test2           | table | postgres | permanent   | heap          | 0 bytes    |
(22 rows)
```
### Проверка степени отказоустойчивости:
###### Гасим ноду citus22 :
```
# На координаторе citus.shard_replication_factor = '2':
bank=# \dt+
                                     List of relations
 Schema |   Name   | Type  |  Owner   | Persistence | Access method |  Size   | Description
--------+----------+-------+----------+-------------+---------------+---------+-------------
 public | accounts | table | postgres | permanent   | heap          | 16 kB   |
 public | test2    | table | postgres | permanent   | heap          | 0 bytes |
(2 rows)
bank=# select * from accounts;
WARNING:  connection to the remote node citus22:5432 failed with the following error: server closed the connection unexpectedly
        This probably means the server terminated abnormally
        before or while processing the request.
could not send SSL negotiation packet: Success
                  id                  | balance
--------------------------------------+---------
 94b52377-aa56-49dd-bf1c-04a8fbf6317a |     380
 2db45df1-168f-4399-9866-1bd59a867dbf |   20000
 a83d6e48-f037-416e-8bf4-4570381bf58b |     500
 33b62bd8-216b-4d8f-8c01-9e666a7a6652 |   55000
 dedba0ca-4b09-4d69-b4ba-fb89278224e9 | 1000.50
(5 rows)
```
##### Гасим ноду citus2 :
```
# На координаторе citus.shard_replication_factor = '1':
bank=# select * from accounts;
# После долгого ожидания:
WARNING:  could not issue cancel request
DETAIL:  Client error: PQcancel() -- connect() failed: error 110
ERROR:  terminating connection due to administrator command
CONTEXT:  while executing command on citus2:5432
bank=#
```
###### Это означает, что при падении 1 воркера теряется все. 
```
# Урать остановленную ноду:
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus44   |      5432
 citus33   |      5432
 citus22   |      5432
(3 rows)
SELECT citus_disable_node('citus22', 5432);
NOTICE:  Node citus22:5432 has active shard placements. Some queries may fail af ter this operation. Use SELECT citus_activate_node('citus22', 5432) to activate this node back.
 citus_disable_node
--------------------
(1 row)

bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus44   |      5432
 citus33   |      5432
(2 rows)
SELECT citus_activate_node('citus55', 5432);
 node_name | node_port
-----------+-----------
 citus44   |      5432
 citus33   |      5432
 citus22   |      5432
(3 rows)
SELECT master_remove_node('citus55', 5432);

```
# Добавляем ноду citus55:
```
\c bank
SELECT * FROM master_add_node('citus55', 5432);
bank=# SELECT * FROM master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 citus44   |      5432
 citus33   |      5432
 citus55   |      5432
(3 rows)

bank=# SELECT rebalance_table_shards('accounts');
server closed the connection unexpectedly This probably means the server terminated abnormally before or while processing the request.
The connection to the server was lost. Attempting reset: Failed.
bank=# SELECT rebalance_table_shards('accounts');
ОШИБКА:  connection to the remote node localhost:5432 failed with the following error: fe_sendauth: no password supplied
# .pgpass
ЗАМЕЧАНИЕ:  Moving shard 102008 from citus3:5432 to citus4:5432 ...
ОШИБКА:  не удалось создать слот репликации "citus_shard_move_subscription_10": ОШИБКА:  для логического декодирования требуется wal_level >= logical
КОНТЕКСТ:  while executing command on citus4:5432
while executing command on localhost:5432
```
```

# Согласно документации: Включить логическую репликацию в исходной базе данных:
vim /etc/postgresql/14/main/postgresql.conf
wal_level = logical
max_replication_slots = 5
max_wal_senders = 5 
```
```
show wal_level;
postgres=# ALTER SYSTEM SET wal_level = logical;
```

###### Создание ПУБЛИКАЦИИ на координаторе citus1
```
CREATE PUBLICATION accounts_pub FOR TABLE accounts;
\dRp+
Публикация accounts_pub
 Владелец | Все таблицы | Добавления | Изменения | Удаления | Опустошения | Через корень
----------+-------------+------------+-----------+----------+-------------+--------------
 postgres | f           | t          | t         | t        | t           | f
Таблицы:
    "public.accounts"
```
###### Cоздание ПОДПИСКИ на citus3 без переноса начальных данных с CITUS1:
```
# БД и таблица accounts должны существовать:
CREATE SUBSCRIPTION accounts_sub
CONNECTION 'host=10.128.0.30 port=5432 user=postgres password=12345 dbname=bank' 
PUBLICATION accounts_pub WITH (copy_data = false);


CREATE SUBSCRIPTION accounts_sub
CONNECTION 'host=192.168.5.172 port=5432 user=postgres password=12345 dbname=bank' 
PUBLICATION accounts_pub WITH (copy_data = false);
# copy_data = false - создание подписки без переноса исходных данных.
```
###### РЕБАЛАНС
```
bank=# SELECT rebalance_table_shards('accounts');
NOTICE:  Moving shard 102008 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102009 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102010 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102011 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102012 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102013 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102014 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102015 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102016 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102017 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102018 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102019 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102020 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102021 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102022 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102023 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102024 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102025 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102026 from citus3:5432 to citus4:5432 ...
NOTICE:  Moving shard 102027 from citus2:5432 to citus4:5432 ...
NOTICE:  Moving shard 102028 from citus3:5432 to citus4:5432 ...
 rebalance_table_shards
------------------------

(1 row)


```

```
CREATE TABLE test (
    Region VARCHAR(50),
    Country VARCHAR(50),
    ItemType VARCHAR(50),
    SalesChannel VARCHAR(20),
    OrderPriority VARCHAR(10),
    OrderDate VARCHAR(10),
    OrderID int,
    ShipDate VARCHAR(10),
    UnitsSold int,
    UnitPrice decimal(12,2),
    UnitCost decimal(12,2),
    TotalRevenue decimal(12,2),
    TotalCost decimal(12,2),
    TotalProfit decimal(12,2)
);
ALTER TABLE test ADD PRIMARY KEY (OrderID);
SELECT create_distributed_table('test', 'orderid');


```















