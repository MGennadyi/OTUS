# Greenplum
###### Оф.требования ubuntu-18, 20 нет.
```
cat /etc/lsb-release
# или
cat /etc/os-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=20.04
DISTRIB_CODENAME=focal
```
```
vim /etc/hosts
10.128.0.51 gp1.ru-central1.internal gp1
10.128.0.52 gp2.ru-central1.internal gp2
10.128.0.53 gp3.ru-central1.internal gp3
10.128.0.54 gp4.ru-central1.internal gp4
```
###### Правим репозиторий:
```
sudo vim /etc/apt/sources.list
deb http://security.ubuntu.com/ubuntu xenial-security main
deb http://ppa.launchpad.net/greenplum/db/ubuntu bionic main
deb http://ru.archive.ubuntu.com/ubuntu bionic main
```
```
# Добавим ключики:
gpg --keyserver keyserver.ubuntu.com --recv 3C6FDC0C01D86213
gpg --export --armor 3C6FDC0C01D86213 | sudo apt-key add -

sudo apt update && sudo apt upgrade -y -q
```
```
# Создаем группу, юзера, пароль:
sudo groupadd gpadmin
sudo useradd gpadmin -r -m -g gpadmin
echo gpadmin:gpadmin123 | sudo chpasswd
# или так: 
sudo passwd gpadmin
```
```
# создаем ssh ключ (без фразы)  для связывания ВМ между собой:
sudo -u gpadmin ssh-keygen -t rsa -b 4096 -q -f /home/gpadmin/.ssh/id_rsa -N ''
# Даем пользователю gpadmin право на sudo:
sudo usermod -aG sudo gpadmin
```
###### Аристов: Установка из пакетов не сработает, т.к. на 20 ubuntu нет пакетов.
###### Создаем bash-скрипт
```
vim greenplum.sh
```
```
#! /bin/bash

# Check we are a root
if [ "$EUID" -ne 0 ]
    then echo "Please run this script as root"
    exit
fi

REPO="/etc/apt/sources.list.d/greenplum-ubuntu-db-bionic.list"
PIN="/etc/apt/preferences.d/99-greenplum"

echo "Add required repositories"
touch \$REPO
cat > \$REPO <<REPOS
deb http://ppa.launchpad.net/greenplum/db/ubuntu bionic main
deb http://ru.archive.ubuntu.com/ubuntu bionic main
REPOS

echo "Configure repositories"
touch \$PIN
cat > \$PIN <<PIN_REPO
Package: *
Pin: release v=18.04
Pin-Priority: 1
PIN_REPO

echo "Repositories described in \$REPO"
echo "Repositories configuration in \$PIN"
echo "Installing greenplum"
sudo apt update && sudo apt install greenplum-db-6 -y
```
```
# Права на выполнение и выполняем:
chmod +x greenplum.sh
sudo ./greenplum.sh
```
```
# Смотрим, что установилось:
dpkg-query -l
# Появился пакет: greenplum-db-6 
```
```
# выдадим права -R на каталог по умолчанию:
sudo find / -name "greenplum*"
# Нужный ответ: /opt/greenplum-db-6.21.0/greenplum_path.sh
sudo chown -R gpadmin:gpadmin /opt/greenplum*
```
```
sudo su gpadmin
$
bash
To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.
gpadmin@gp2:/home/mgb$
source /opt/greenplum-db-6.21.0/greenplum_path.sh
which gpssh
# Ответ:
/opt/greenplum-db-6.21.0/bin/gpssh
gpadmin@gp2:/home/mgb$
```
###### Выполняем скрипт первоначальной настройки через 'source' встройка кода в bach :
```
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
$PATH
sudo -u gpadmin source /opt/greenplum-db-6.21.0/greenplum_path.sh
# Ответ: sudo: source: command not found
# Прописалась ли переменная?:
which gpssh
# Ответа не должно быть
bash
gpadmin@gp1:vim /home/gpadmin/.bashrc
# Нужно наш sources прописать в .bashrc:
source /opt/greenplum-db-6.21.0/greenplum_path.sh
which gpssh
# Ответ: /opt/greenplum-db-6.21.0/bin/gpssh
exit
```
```
# Т.О. прописываем код в наш bash:
# По ниже команде по умолчанию попадаем не в bash, а в dash!:
root@gp:/home/mgb# sudo su gpadmin
# По этому выполняем:
$ bash
gpadmin@gp:/home/mgb$ source /opt/greenplum-db-6.21.0/greenplum_path.sh
gpadmin@gp:/home/mgb$ which gpssh
/opt/greenplum-db-6.21.0/bin/gpssh
exit
# Что бы не dash, а bash для этого:
sudo dpkg-reconfigure dash
pass: 12345
# Попадаем в окно конфига dash, отвечаем:
no
exit
# Если теперь зайти под пользователем gpadmin:
root@gp:/home/mgb# sudo su gpadmin
# Будет:
sh-5.0$
echo $shell
# Не помогло сменить dash на bash, т.к теперь идет запуск просто shell. Для bash меняем вручную:
chsh
Password: 12345
# Ответ:
        Login Shell [/bin/sh]: 
# Ввести:
/bin/bash
sh-5.0$
# теперь команды из-под пользователя gpadmin будут обрабатываться bash. 
# Проделаем то же самое на 2-4 ВМ, из под gpadmin:
sudo su gpadmin
# Прописываем source в bashrc:
echo "source /opt/greenplum-db-6.21.0/greenplum_path.sh" >> ~/.bashrc
# Меняем shell на bash;
chsh -s /bin/bash
pass: 12345
# Проверим, что все работает:
which gpssh
# Ответ:
/opt/greenplum-db-6.21.0/bin/gpssh
```
###### По документации Прописать ssh друг у друга. Не работает:
```
sudo apt install ssh
ssh-copy-id gp2
yes
Permission denied (publickey).
```
```
# Аристов: идем рекомендованным производителем путем, вкл. вход по паролю (потом лучше отключить):
sudo vim /etc/ssh/sshd_config
# Включ. временно вход по паролю:
sudo vim /etc/ssh/sshd_config
PasswordAuthentication no -> yes (в самом низу)
sudo systemctl restart sshd
# Проделать на остальных вм
exit
root@gp:/home/mgb# sudo su gpadmin
gpadmin@gp:/home/mgb$ sudo vim /etc/ssh/sshd_config
[sudo] password for gpadmin:
gpadmin@gp:/home/mgb$ sudo systemctl restart sshd
# C gp1 Раскладываем ключ по другим ВМ из под gpadmin:
ssh-copy-id gp2
yes
pass: 12345
ssh-copy-id gp3
ssh-copy-id gp4
# C gp2 Раскладываем ключ по другим ВМ:
ssh-copy-id gp1
ssh-copy-id gp3
ssh-copy-id gp4
# C gp3 Раскладываем ключ по другим ВМ:
ssh-copy-id gp1
ssh-copy-id gp2
ssh-copy-id gp4
# C gp4 Раскладываем ключ по другим ВМ:
ssh-copy-id gp1
ssh-copy-id gp2
ssh-copy-id gp3
```
```
# На каждой ноде прописываем: Если в домашнем каталоге gpadmin, то: 
sudo vim ~/hostfile_exkeys
# иначе:
sudo vim /home/gpadmin/hostfile_exkeys
gp1
gp2
gp3
gp4
```
###### Убеждаемся, что каждая нода видит других через утилиту gpssh-exkeys:
```
sudo vim /etc/apt/sources.list
sudo apt install libssl1.0.0 -y
cd ~
gpssh-exkeys -f hostfile_exkeys
# Ответ:
[STEP 1 of 5] create local ID and authorize on local host
  ... /home/gpadmin/.ssh/id_rsa file exists ... key generation skipped

[STEP 2 of 5] keyscan all hosts and update known_hosts file

[STEP 3 of 5] retrieving credentials from remote hosts
  ... send to gp2
  ... send to gp3
  ... send to gp4

[STEP 4 of 5] determine common authentication file content

[STEP 5 of 5] copy authentication files to all remote hosts
  ... finished key exchange with gp2
  ... finished key exchange with gp3
  ... finished key exchange with gp4

[INFO] completed successfully
gpadmin@gp1:~$
```
##### На мастере подтвердить установку ssh:
```
gpssh -f hostfile_exkeys -e 'ls -l /opt/greenplum-db-6.21.0'
# Можно увидеть на каких нодах какие каталоги доступны
```
###### На всех нодах создать каталог, где будут лежать наши данные: 'Data Storage Areas':
```
sudo mkdir -p /data/master
sudo chown gpadmin:gpadmin /data/master
```
```
# На ноде 2:
sudo mkdir -p /data/master
sudo chown gpadmin:gpadmin /data/master

# На ноде 3-4, отличаются доп.созданием /mirror:
sudo mkdir -p /data/primary
sudo mkdir -p /data/mirror
sudo chown -R gpadmin /data/*
```
###### Валидировать нашу установку
###### Инициализация:
```
# На всех хостах создаем конфиг-файл, где указываем сегменты:
cd ~
vim ~/hostfile_gpinitsystem
# или
sudo vim /home/gpadmin/hostfile_gpinitsystem
gp3
gp4
```
###### При инициализации системы указать, где второй мастер сразу включаем мирроринг и второго мастера
```
cd ~
gpinitsystem -c gpconfigs/gpinitsystem_config -h hostfile_gpinitsystem -s gp2 --mirror-mode=spread
# Не заработает по причине: колич-во ностов должно быть больше кол-ва сегментов.
cd ~
vim ~/hostfile
vim /home/gpadmin/gpconfigs/gpinitsystem_config
# Задаем:
# Кто у нас мастер:
MASTER_HOSTNAME=gp1
# Каталог, где лежат данные:
MASTER_DIRECTORY=/data/master
MIRROR_PORT_BASE=7000
declare -a MIRROR_DATA_DIRECTORY=(/data/mirror)
# Заменить не верные директории без 1:
declare -a DATA_DIRECTORY=(/data1/primary /data1/primary /data1/primary /data2/primary /data2/primary /data2/primary)
declare -a DATA_DIRECTORY=(/data/primary)
```
```
gpadmin@gp1: mkdir /home/gpadmin/gpconfigs
cp $GPHOME/docs/cli_help/gpconfigs/gpinitsystem_config \
     /home/gpadmin/gpconfigs/gpinitsystem_config
cd ~
vim gpconfigs/gpinitsystem_config


```
###### Кол-во хостов должно быть больше кол-ва сегментов
```
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-   Successful segment starts                                            = 2
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-   Failed segment starts                                                = 0
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-   Skipped segment starts (segments are marked down in configuration)   = 0
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-----------------------------------------------------
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-Successfully started 2 of 2 segment instances
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-----------------------------------------------------
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-Starting Master instance gp1 directory /data/master/gpseg-1
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-Command pg_ctl reports Master gp1 instance active
20220727:11:16:38:003217 gpstart:gp1:gpadmin-[INFO]:-Connecting to dbname='template1' connect_timeout=15
20220727:11:16:39:003217 gpstart:gp1:gpadmin-[INFO]:-No standby master configured.  skipping...
20220727:11:16:39:003217 gpstart:gp1:gpadmin-[INFO]:-Database successfully started
20220727:11:16:39:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Completed restart of Greenplum instance in production mode
20220727:11:16:39:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Starting initialization of standby master gp2
20220727:11:16:39:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Validating environment and parameters for standby initialization...
20220727:11:16:39:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Checking for data directory /data/master/gpseg-1 on gp2
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:------------------------------------------------------
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Greenplum standby master initialization parameters
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:------------------------------------------------------
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Greenplum master hostname               = gp1
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Greenplum master data directory         = /data/master/gpseg-1
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Greenplum master port                   = 5432
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Greenplum standby master hostname       = gp2
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Greenplum standby master port           = 5432
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Greenplum standby master data directory = /data/master/gpseg-1
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Greenplum update system catalog         = On
20220727:11:16:40:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Syncing Greenplum Database extensions to standby
20220727:11:16:41:003306 gpinitstandby:gp1:gpadmin-[INFO]:-The packages on gp2 are consistent.
20220727:11:16:41:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Adding standby master to catalog...
20220727:11:16:41:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Database catalog updated successfully.
20220727:11:16:41:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Updating pg_hba.conf file...
20220727:11:16:42:003306 gpinitstandby:gp1:gpadmin-[INFO]:-pg_hba.conf files updated successfully.
20220727:11:16:46:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Starting standby master
20220727:11:16:46:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Checking if standby master is running on host: gp2  in directory: /data/master/gpseg-1
20220727:11:16:50:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Cleaning up pg_hba.conf backup files...
20220727:11:16:50:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Backup files of pg_hba.conf cleaned up successfully.
20220727:11:16:50:003306 gpinitstandby:gp1:gpadmin-[INFO]:-Successfully created standby master on gp2
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Successfully completed standby master initialization
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Scanning utility log file for any warning messages
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[WARN]:-*******************************************************
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[WARN]:-Scan of log file indicates that some warnings or errors
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[WARN]:-were generated during the array creation
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Please review contents of log file
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-/home/gpadmin/gpAdminLogs/gpinitsystem_20220727.log
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-To determine level of criticality
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[WARN]:-*******************************************************
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Greenplum Database instance successfully created
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-------------------------------------------------------
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-To complete the environment configuration, please
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-update gpadmin .bashrc file with the following
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-1. Ensure that the greenplum_path.sh file is sourced
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-2. Add "export MASTER_DATA_DIRECTORY=/data/master/gpseg-1"
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-   to access the Greenplum scripts for this instance:
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-   or, use -d /data/master/gpseg-1 option for the Greenplum scripts
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-   Example gpstate -d /data/master/gpseg-1
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Script log file = /home/gpadmin/gpAdminLogs/gpinitsystem_20220727.log
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-To remove instance, run gpdeletesystem utility
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Standby Master gp2 has been configured
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-To activate the Standby Master Segment in the event of Master
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-failure review options for gpactivatestandby
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-------------------------------------------------------
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-The Master /data/master/gpseg-1/pg_hba.conf post gpinitsystem
20220727:11:16:50:000795 gpinitsystem:gp1:gpadmin-[INFO]:-has been configured to allow all hosts within this new
20220727:11:16:51:000795 gpinitsystem:gp1:gpadmin-[INFO]:-array to intercommunicate. Any hosts external to this
20220727:11:16:51:000795 gpinitsystem:gp1:gpadmin-[INFO]:-new array must be explicitly added to this file
20220727:11:16:51:000795 gpinitsystem:gp1:gpadmin-[INFO]:-Refer to the Greenplum Admin support guide which is
20220727:11:16:51:000795 gpinitsystem:gp1:gpadmin-[INFO]:-located in the /opt/greenplum-db-6.21.0/docs directory
20220727:11:16:51:000795 gpinitsystem:gp1:gpadmin-[INFO]:-------------------------------------------------------
gpadmin@gp1:~$
```
###### Проверка состояния
```
gpstate
gpadmin@gp1:/home/mgb$ gpstate
# Просмотр статуса на мастере на работающем кластере:
20220729:13:06:36:010166 gpstate:gp1:gpadmin-[INFO]:-Starting gpstate with args:
20220729:13:06:36:010166 gpstate:gp1:gpadmin-[CRITICAL]:-gpstate failed. (Reason='Environment Variable MASTER_DATA_DIRECTORY not set!') exiting...
```
```
# Проверка состояния на мастере через внесения изменений на всех нодах:
vim ~/.bashrc
MASTER_DATA_DIRECTORY=/data/master/gpseg-1
export MASTER_DATA_DIRECTORY
exit
gpstate
```
###### Ответ:
```
20220729:13:36:17:000930 gpstate:gp1:gpadmin-[INFO]:-Starting gpstate with args:
20220729:13:36:17:000930 gpstate:gp1:gpadmin-[INFO]:-local Greenplum Version: 'postgres (Greenplum Database) 6.21.0 build commit:d0087e3b24c54d203ca8bb315559205f13cd6393 Open Source'
20220729:13:36:17:000930 gpstate:gp1:gpadmin-[CRITICAL]:-gpstate failed. (Reason='could not connect to server: Connection refused
        Is the server running on host "localhost" (::1) and accepting
        TCP/IP connections on port 5432?
could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?
') exiting...
```
##### После рестарта всех нод, на мастер-ноде:
```
gpstart
# Прописать 
vim ~/.bashrc
MASTER_DATA_DIRECTORY=/opt/data/master/gpseg-1
export MASTER_DATA_DIRECTORY
echo ~/.bashrc
# Ответ:
20220801:06:36:10:001460 gpstart:gp1:gpadmin-[INFO]:-Starting gpstart with args:
20220801:06:36:10:001460 gpstart:gp1:gpadmin-[INFO]:-Gathering information and validating the environment...
20220801:06:36:10:001460 gpstart:gp1:gpadmin-[INFO]:-Greenplum Binary Version: 'postgres (Greenplum Database) 6.21.0 build commit:d0087e3b24c54d203ca8bb315559205f13cd6393 Open Source'
20220801:06:36:10:001460 gpstart:gp1:gpadmin-[INFO]:-Greenplum Catalog Version: '301908232'
20220801:06:36:10:001460 gpstart:gp1:gpadmin-[INFO]:-Starting Master instance in admin mode
20220801:06:36:11:001460 gpstart:gp1:gpadmin-[INFO]:-Obtaining Greenplum Master catalog information
20220801:06:36:11:001460 gpstart:gp1:gpadmin-[INFO]:-Obtaining Segment details from master...
20220801:06:36:12:001460 gpstart:gp1:gpadmin-[INFO]:-Setting new master era
20220801:06:36:12:001460 gpstart:gp1:gpadmin-[INFO]:-Master Started...
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-Shutting down master
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:---------------------------
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-Master instance parameters
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:---------------------------
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-Database                 = template1
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-Master Port              = 5432
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-Master directory         = /data/master/gpseg-1
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-Timeout                  = 600 seconds
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-Master standby start     = On
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:---------------------------------------
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-Segment instances that will be started
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:---------------------------------------
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-   Host   Datadir                Port   Role
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-   gp3    /data/primary/gpseg0   6000   Primary
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-   gp4    /data/mirror/gpseg0    7000   Mirror
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-   gp4    /data/primary/gpseg1   6000   Primary
20220801:06:36:13:001460 gpstart:gp1:gpadmin-[INFO]:-   gp3    /data/mirror/gpseg1    7000   Mirror

Continue with Greenplum instance startup Yy|Nn (default=N):
> y
20220801:06:36:27:001460 gpstart:gp1:gpadmin-[INFO]:-Commencing parallel primary and mirror segment instance startup, please wait...
....
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-Process results...
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-----------------------------------------------------
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-   Successful segment starts                                            = 4
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-   Failed segment starts                                                = 0
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-   Skipped segment starts (segments are marked down in configuration)   = 0
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-----------------------------------------------------
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-Successfully started 4 of 4 segment instances
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-----------------------------------------------------
20220801:06:36:31:001460 gpstart:gp1:gpadmin-[INFO]:-Starting Master instance gp1 directory /data/master/gpseg-1
20220801:06:36:32:001460 gpstart:gp1:gpadmin-[INFO]:-Command pg_ctl reports Master gp1 instance active
20220801:06:36:32:001460 gpstart:gp1:gpadmin-[INFO]:-Connecting to dbname='template1' connect_timeout=15
20220801:06:36:33:001460 gpstart:gp1:gpadmin-[INFO]:-Starting standby master
20220801:06:36:33:001460 gpstart:gp1:gpadmin-[INFO]:-Checking if standby master is running on host: gp2  in directory: /data/master/gpseg-1
20220801:06:36:37:001460 gpstart:gp1:gpadmin-[INFO]:-Database successfully started
```
###### Вход в postgresql:
```
gpadmin@gp1:~$ psql
psql: FATAL:  database "gpadmin" does not exist
# Правильный вход:
gpadmin@gp1:~$ psql -d postgres
psql (9.4.26)
postgres=# \l
                               List of databases
   Name    |  Owner  | Encoding |  Collate   |   Ctype    |  Access privileges
-----------+---------+----------+------------+------------+---------------------
 postgres  | gpadmin | UTF8     | en_US.utf8 | en_US.utf8 |
 template0 | gpadmin | UTF8     | en_US.utf8 | en_US.utf8 | =c/gpadmin         +
           |         |          |            |            | gpadmin=CTc/gpadmin
 template1 | gpadmin | UTF8     | en_US.utf8 | en_US.utf8 | =c/gpadmin         +
           |         |          |            |            | gpadmin=CTc/gpadmin
```
```
# Зальем данные:
cd ~
gpadmin@gp1:~$ cd $HOME && wget --quiet https://edu.postgrespro.ru/demo_small.zip && unzip demo_small.zip && psql -d postgres < demo_small.sql
# При загрузке будут ошибки со старыми индексами, т.к. v.9.4.26 много чего не поддерживает.
\l+
```
```
psql -d demo
\l+
demo=# \dt+
# Ответ:
No relations found.
# Надо смотреть схему bookings, которая не включена в путь:
\dt+ bookings.*
```
###### Создаем flights2, но сделаем ее DISTRIBUTED, т.е. она разъеддится по нодам. Партиционируем по аэропортам вылета+подпартиции по месяцам:
```
CREATE TABLE flights2 (
    flight_id integer NOT NULL,
    flight_no character(6) NOT NULL,
    scheduled_departure timestamp with time zone NOT NULL,
    scheduled_arrival timestamp with time zone NOT NULL,
    departure_airport character(3) NOT NULL,
    arrival_airport character(3) NOT NULL,
    status character varying(20) NOT NULL,
    aircraft_code character(3) NOT NULL,
    actual_departure timestamp with time zone,
    actual_arrival timestamp with time zone,
    CONSTRAINT flights_check CHECK ((scheduled_arrival > scheduled_departure)),
    CONSTRAINT flights_check1 CHECK (((actual_arrival IS NULL) OR ((actual_departure IS NOT NULL) AND (actual_arrival IS NOT NULL) AND (actual_arrival > actual_departure)))),
    CONSTRAINT flights_status_check CHECK (((status)::text = ANY (ARRAY[('On Time'::character varying)::text, ('Delayed'::character varying)::text, ('Departed'::character varying)::text, ('Arrived'::character varying)::text, ('Scheduled'::character varying)::text, ('Cancelled'::character varying)::text])))
)
DISTRIBUTED RANDOMLY
PARTITION BY RANGE (scheduled_departure)
SUBPARTITION BY LIST (status)
SUBPARTITION TEMPLATE
( SUBPARTITION stat_onti VALUES ('On Time'), 
  SUBPARTITION stat_dely VALUES ('Delayed'), 
  SUBPARTITION stat_depd VALUES ('Departed'), 
  SUBPARTITION stat_arrd VALUES ('Arrived'), 
  SUBPARTITION stat_schd VALUES ('Scheduled'), 
  SUBPARTITION stat_canc VALUES ('Cancelled'),
  DEFAULT SUBPARTITION stat_othr)
  (START (TIMESTAMP '2020-01-01 00:00:00+00') INCLUSIVE
   END (TIMESTAMP '2022-01-01 00:00:00+00') EXCLUSIVE
   EVERY (INTERVAL '1 month'), 
   DEFAULT PARTITION dep_othr );
```
```
# select * from flights2; - Данных не обнаружим. Заполняем данными: 
INSERT INTO flights2 SELECT * FROM bookings.flights;
# Ответ: INSERT 0 33121
select * from flights2;
# Подвисает
select * from flights2 where scheduled_departure = '2016-10-14 07:05:00+00'; 
# Ответ:
WARNING:  interconnect may encountered a network error, please check your network  (seg1 slice1 10.128.0.54:6000 pid=1079)
DETAIL:  Failed to send packet (seq 1) to 127.0.1.1:58365 (pid 1646 cid -1) after 100 retries.
WARNING:  interconnect may encountered a network error, please check your network  (seg0 slice1 10.128.0.53:6000 pid=1092)
DETAIL:  Failed to send packet (seq 1) to 127.0.1.1:58365 (pid 1646 cid -1) after 100 retries.


```


